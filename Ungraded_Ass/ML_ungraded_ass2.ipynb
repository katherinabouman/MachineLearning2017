{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC ML LabExercise - Univariate Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement univariate linear regression. You will implement a gradient descent procedure to iteratively search for the solution. \n",
    "$$\n",
    "\\newcommand{\\ls}[1]{{}^{(#1)}}\n",
    "\\renewcommand{\\v}[1]{\\boldsymbol{#1}}\n",
    "\\renewcommand{\\T}{{}^T}\n",
    "\\newcommand{\\matvec}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data $(x\\ls 1,y\\ls 1),\\ldots,(x\\ls m, y\\ls m)$ where the $x$ values are the independent variables, these values are error free. The dependent values $y$ do contain errors.\n",
    "\n",
    "Linear regression fits a model function (*hypothesis*) $h_{\\v\\theta}(x)$ such that the sum of squared errors is minimized:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\v\\theta}(x\\ls i) - y\\ls i)^2\n",
    "$$\n",
    "Linear regression is called *linear* regression because we assume the hypothesis function $h_{\\v\\theta}$ is linear in its parameters $\\v\\theta$:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\theta_0 \\phi_0(x) + \\cdots + \\theta_n \\phi_n(x)\n",
    "$$\n",
    "where $\\phi_0,\\ldots,\\phi_n$ are arbitrary functions in $x$. In case we write:\n",
    "$$\n",
    "\\v x = \\matvec{\\phi_0(x)\\\\\\vdots\\\\\\phi_n(x)}\n",
    "$$\n",
    "the hypothesis function becomes:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\v\\theta\\T \\v x\n",
    "$$\n",
    "and the cost function is:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i)^2\n",
    "$$\n",
    "The gradient is given by:\n",
    "$$\n",
    "\\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta} =\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i) \\v x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Practice I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple example. We will generate data with:\n",
    "$$\n",
    "   y\\ls i = a x\\ls i + b + R\n",
    "$$\n",
    "where $R$ is a random variable, i.e. its value is not exactly\n",
    "known. We assume here that $R$ is normally distributed with mean zero\n",
    "and standard deviation 0.3.\n",
    "\n",
    "We collect all values $x\\ls i$ for $i=1,\\ldots,m$ in an array of shape ``(m,)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGIIbAArzf96ZCMlsrwSsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b1cf110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 100;\n",
    "a = 0.5\n",
    "b = 2\n",
    "x = linspace(0,10,m)\n",
    "y = a * x + b + 0.3 * random.randn(m)\n",
    "plot(x, y, 'r+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that $x\\ls i$ is stored in ``x[i]`` (and equivalently for $y$\n",
    "and ``y``).\n",
    "\n",
    "In this case we want to fit a model of the form $h_{\\v\\theta}(x)=a x + b$\n",
    "to the data. Note that with \n",
    "$$\n",
    "   \\v x = \\matvec{1\\\\x}\n",
    "$$\n",
    "(i.e. with $\\phi_0(x)=1$ and $\\phi_1(x)=x$) we have:\n",
    "$$\n",
    "   h_{\\v\\theta}(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "where $\\theta_0$ is $a$ and $\\theta_0$ is $b$. A constant function\n",
    "$\\phi_0$ in a linear hypothesis (*linear in its parameters!*) is often\n",
    "called a bias term of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``cost(theta, x, y)`` that calculates the cost. Note that ``x`` is the vector with all $x\\ls i$-values and ``y`` is the vector with all $y\\ls i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h_func(t0,t1, x):\n",
    "    return t0 + t1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.045, 0.045555120950630322)\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "def cost(theta0, theta1, x, y):\n",
    "    m = len(x)\n",
    "    h = h_func(theta0, theta1, x)\n",
    "    result = sum((h - y)**2) / (2 * m)\n",
    "    return result\n",
    "\n",
    "print(0.3**2/2, cost(2, 0.5, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 16.7 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 cost(2, 0.5, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function called with ``cost(b,a,x,y)`` (where ``b``, ``a``,\n",
    "``x`` and ``y``) are defined as in the previous code snippet,\n",
    "should a return a value that is close to $0.3^2/2$ (For extra\n",
    "points: can you prove this?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``theta0, theta1 = gradDescentStep(theta0, theta1, x, y)`` that does the calculations for one gradient descent step. In this function we use the Python possibility to return a tuple of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derv(h, m, x, y):\n",
    "    return dot((h - y),x)/ m  #You need dot product, because you want scalar out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescentStep(learningrate, theta0, theta1, x, y):\n",
    "    h = h_func(theta0, theta1, x)\n",
    "    derv = sum(h - y) / m\n",
    "    temp0 = theta0 - learningrate * derv (h, m, 1, y) # x0 = 1\n",
    "    temp1 = theta1 - learningrate * derv (h, m, x, y)\n",
    "    theta0 = temp0\n",
    "    theta1 = temp1\n",
    "    return theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with values ``theta0 = theta1 = 0``. Calculate the costfor these values. After the gradient descent step, using ``learningrate=0.01``, resulting in new theta values again calculate the cost. If all went well the cost should have decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b6bf33498ce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtheta0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcostbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradDescentStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcostafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcostbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'>='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcostafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-10241028ebeb>\u001b[0m in \u001b[0;36mgradDescentStep\u001b[0;34m(learningrate, theta0, theta1, x, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mderv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtemp0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearningrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mderv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# x0 = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtemp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearningrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mderv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtheta0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "theta0 = theta1 = 0\n",
    "costbefore = cost(theta0, theta1, x, y)\n",
    "theta0, theta1 = gradDescentStep(0.01, theta0, theta1, x, y)\n",
    "costafter = cost(theta0, theta1, x, y)\n",
    "print(costbefore, '>=', costafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
