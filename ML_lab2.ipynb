{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC ML LabExercise - Univariate Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement univariate linear regression. You will implement a gradient descent procedure to iteratively search for the solution. \n",
    "$$\n",
    "\\newcommand{\\ls}[1]{{}^{(#1)}}\n",
    "\\renewcommand{\\v}[1]{\\boldsymbol{#1}}\n",
    "\\renewcommand{\\T}{{}^T}\n",
    "\\newcommand{\\matvec}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data $(x\\ls 1,y\\ls 1),\\ldots,(x\\ls m, y\\ls m)$ where the $x$ values are the independent variables, these values are error free. The dependent values $y$ do contain errors.\n",
    "\n",
    "Linear regression fits a model function (*hypothesis*) $h_{\\v\\theta}(x)$ such that the sum of squared errors is minimized:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\v\\theta}(x\\ls i) - y\\ls i)^2\n",
    "$$\n",
    "Linear regression is called *linear* regression because we assume the hypothesis function $h_{\\v\\theta}$ is linear in its parameters $\\v\\theta$:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\theta_0 \\phi_0(x) + \\cdots + \\theta_n \\phi_n(x)\n",
    "$$\n",
    "where $\\phi_0,\\ldots,\\phi_n$ are arbitrary functions in $x$. In case we write:\n",
    "$$\n",
    "\\v x = \\matvec{\\phi_0(x)\\\\\\vdots\\\\\\phi_n(x)}\n",
    "$$\n",
    "the hypothesis function becomes:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\v\\theta\\T \\v x\n",
    "$$\n",
    "and the cost function is:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i)^2\n",
    "$$\n",
    "The gradient is given by:\n",
    "$$\n",
    "\\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta} =\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i) \\v x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Practice I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple example. We will generate data with:\n",
    "$$\n",
    "   y\\ls i = a x\\ls i + b + R\n",
    "$$\n",
    "where $R$ is a random variable, i.e. its value is not exactly\n",
    "known. We assume here that $R$ is normally distributed with mean zero\n",
    "and standard deviation 0.3.\n",
    "\n",
    "We collect all values $x\\ls i$ for $i=1,\\ldots,m$ in an array of shape ``(m,)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAbm/wFUSNQ1FEkr+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117adce50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 100;\n",
    "a = 0.5\n",
    "b = 2\n",
    "x = linspace(0,10,m)\n",
    "y = a * x + b + 0.3 * random.randn(m)\n",
    "plot(x, y, 'r+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that $x\\ls i$ is stored in ``x[i]`` (and equivalently for $y$\n",
    "and ``y``).\n",
    "\n",
    "In this case we want to fit a model of the form $h_{\\v\\theta}(x)=a x + b$\n",
    "to the data. Note that with \n",
    "$$\n",
    "   \\v x = \\matvec{1\\\\x}\n",
    "$$\n",
    "(i.e. with $\\phi_0(x)=1$ and $\\phi_1(x)=x$) we have:\n",
    "$$\n",
    "   h_{\\v\\theta}(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "where $\\theta_0$ is $a$ and $\\theta_0$ is $b$. A constant function\n",
    "$\\phi_0$ in a linear hypothesis (*linear in its parameters!*) is often\n",
    "called a bias term of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``cost(theta, x, y)`` that calculates the cost. Note that ``x`` is the vector with all $x\\ls i$-values and ``y`` is the vector with all $y\\ls i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h_func(t0,t1, x):\n",
    "    return t0 + t1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.045, 0.036359490561387708)\n"
     ]
    }
   ],
   "source": [
    "def cost(theta0, theta1, x, y):\n",
    "    m = len(x)\n",
    "    h = h_func(theta0, theta1, x)\n",
    "    result = sum((h - y)**2) / (2 * m)\n",
    "    return result\n",
    "\n",
    "print(0.3**2/2, cost(2, 0.5, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function called with ``cost(b,a,x,y)`` (where ``b``, ``a``,\n",
    "``x`` and ``y``) are defined as in the previous code snippet,\n",
    "should a return a value that is close to $0.3^2/2$ (For extra\n",
    "points: can you prove this?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``theta0, theta1 = gradDescentStep(theta0, theta1, x, y)`` that does the calculations for one gradient descent step. In this function we use the Python possibility to return a tuple of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derv(h, m, x, y):\n",
    "    return sum(h - y) * x / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescentStep(learningrate, theta0, theta1, x, y):\n",
    "    h = h_func(theta0, theta1, x)\n",
    "    derv = sum(h - y) / m\n",
    "    temp0 = theta0 - learningrate * derv (h, m, 1, y) # x0 = 1\n",
    "    temp1 = theta1 - learningrate * derv (h, m, 1, y)\n",
    "    theta0 = temp0\n",
    "    theta1 = temp1\n",
    "    return theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with values ``theta0 = theta1 = 0``. Calculate the costfor these values. After the gradient descent step, using ``learningrate=0.01``, resulting in new theta values again calculate the cost. If all went well the cost should have decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b6bf33498ce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtheta0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcostbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradDescentStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcostafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcostbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'>='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcostafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-10241028ebeb>\u001b[0m in \u001b[0;36mgradDescentStep\u001b[0;34m(learningrate, theta0, theta1, x, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mderv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtemp0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearningrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mderv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# x0 = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtemp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearningrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mderv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtheta0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "theta0 = theta1 = 0\n",
    "costbefore = cost(theta0, theta1, x, y)\n",
    "theta0, theta1 = gradDescentStep(0.01, theta0, theta1, x, y)\n",
    "costafter = cost(theta0, theta1, x, y)\n",
    "print(costbefore, '>=', costafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
