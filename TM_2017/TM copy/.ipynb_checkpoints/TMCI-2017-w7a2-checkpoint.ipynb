{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:5px 10px 5px 10px\" markdown=\"1\">\n",
    "    <img src=\"images/auc.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right;margin-top:10px\" markdown=\"1\">\n",
    "    <h3><i>Text Mining & Collective Intelligence</i></h3>\n",
    "</div>\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "<center><h1>Semantics with Sparse & Dense Vectors</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<h3>by Gianluca E. Lebani</h3>\n",
    "<h4>• 17 Oct. 2017 •</h4>\n",
    "\n",
    "</center>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Today\n",
    ">\n",
    ">- [Sparse Vectors](#Sparse-Vectors)\n",
    ">\n",
    ">\n",
    ">- [Calculating Cosine Similarities](#Calculating-Cosine-Similarities)\n",
    ">\n",
    ">\n",
    ">- [Visualizing Similarities](#Visualizing-Similarities)\n",
    ">\n",
    ">\n",
    ">- [Evaluating your Model](#Evaluating-your-Model)\n",
    ">\n",
    ">\n",
    ">- [Working with Dense Vectors](#Working-with-Dense-Vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import math \n",
    "\n",
    "from itertools import chain, permutations, product\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import cluster\n",
    "from sklearn import manifold, metrics\n",
    "\n",
    "plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/previously.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from a **pos-tagged and lemmatized** version of the Brown corpus, filtered by removing punctuation marks and \"X\" (i.e. \"OTHER\") words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "un2wn_mapping = {\"VERB\" : \"v\", \"NOUN\" : \"n\", \"ADJ\" : \"a\", \"ADV\" : \"r\"}\n",
    "\n",
    "brown_lemmatized = []\n",
    "\n",
    "for sentence in nltk.corpus.brown.tagged_sents(tagset='universal'):\n",
    "    lemmatized_sentence = []\n",
    "    for w, p in sentence:\n",
    "        if p in [\".\", \"X\"]:\n",
    "            continue\n",
    "        elif p in un2wn_mapping.keys():\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w, pos = un2wn_mapping[p])\n",
    "        else:\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w)\n",
    "        lemmatized_sentence.append(\"-\".join([lemma, p]))\n",
    "        \n",
    "    brown_lemmatized.append(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we've extracted all the **surface co-occurrences** between a noun and another lemma in a ±5 words collocational span..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1277337 target-context pairs\n"
     ]
    }
   ],
   "source": [
    "spansize = 5\n",
    "\n",
    "cooccs_surface = Counter()\n",
    "\n",
    "for sentence in brown_lemmatized:\n",
    "    for i,w in enumerate(sentence):\n",
    "        if w.split(\"-\")[-1] == \"NOUN\":\n",
    "            span_range = range(max(i- spansize, 0), i)  # left side indexes\n",
    "            span_range.extend(range(i+1, min(i + spansize + 1, len(sentence))))  # add the right side indexes\n",
    "            for cw in [sentence[idx] for idx in span_range]:\n",
    "                cooccs_surface[(w, cw)] += 1\n",
    "\n",
    "print len(cooccs_surface), \"target-context pairs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we chose to ignore all those context words that were:\n",
    " \n",
    "    - highly infrequent (i.e. we apply a minimum frequency threshold of **10**)\n",
    "    - instances of **closed class** words \n",
    "    - semantically \"empty\" verbs like \"be\", \"have\", \"do\", \"would\", \"will\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7576 context lemmas\n"
     ]
    }
   ],
   "source": [
    "filtered_brown_lemmas_frequencies = Counter()\n",
    "\n",
    "for k,v in Counter(chain(*brown_lemmatized)).iteritems():\n",
    "    if v >= 10 and \\\n",
    "    k.split(\"-\")[-1] in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]\\\n",
    "    and k.split(\"-\")[0] not in [\"be\", \"have\", \"do\", \"would\", \"will\", \"could\"]:\n",
    "        filtered_brown_lemmas_frequencies[k] = v\n",
    "\n",
    "print len(filtered_brown_lemmas_frequencies), \"context lemmas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we chose to ignore all those target nouns that were:\n",
    " \n",
    "    - highly infrequent (i.e. we apply a minimum frequency threshold of **10**)\n",
    "    - shorter than 3 characters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4045 target nouns\n"
     ]
    }
   ],
   "source": [
    "selected_brown_noun_frequencies = Counter()\n",
    "\n",
    "for k,v in Counter([w for w in chain(*brown_lemmatized) if w.split(\"-\")[-1] == \"NOUN\"]).most_common():\n",
    "    if len(k) < 8:  # i.e. let's ignore lemmas shorted the 3 characters\n",
    "        continue\n",
    "    elif v == 10:\n",
    "        break\n",
    "    else:\n",
    "        selected_brown_noun_frequencies[k] = v\n",
    "\n",
    "print len(selected_brown_noun_frequencies), \"target nouns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537486 selected target-context pairs\n"
     ]
    }
   ],
   "source": [
    "filtered_cooccs_surface = Counter()\n",
    "\n",
    "for k,v in cooccs_surface.iteritems():\n",
    "    if selected_brown_noun_frequencies.has_key(k[0]) and filtered_brown_lemmas_frequencies.has_key(k[1]):\n",
    "        filtered_cooccs_surface[k] = v\n",
    "\n",
    "print len(filtered_cooccs_surface), \"selected target-context pairs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 25px 10px 20px\">\n",
    "    <img src=\"images/your_turn.jpg\" width=\"110\">\n",
    "</div>\n",
    "\n",
    "#### Your Turn.\n",
    "\n",
    "How many noun-context pairings never occurr in our corpus? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we finally **weighted** our raw counts by using the following three association measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "from math import log\n",
    "\n",
    "# All functions receive as arguments:\n",
    "#\n",
    "#   o_11>> the joint frequency of w1 and w2 >> f(w1, w2)\n",
    "#   r_1 >> w1 marginals >> f(w1, *)\n",
    "#   c_1 >> w2 marginals >> f(*, w2)\n",
    "#   n >> total number of possible coccorrencies >> f(*, *)\n",
    "\n",
    "# These arguments can be arranged in a contingency table of the form:\n",
    "#\n",
    "#          w2    ~w2\n",
    "#        ------ ------\n",
    "#    w1 | o_11 | o_12 | = r_1\n",
    "#        ------ ------\n",
    "#   ~w1 | o_21 | o_22 | = r_2\n",
    "#        ------ ------\n",
    "#        = c_1  = c_2   = n\n",
    "\n",
    "\n",
    "def ppmi(o_11, r_1, c_1, n):\n",
    "    \"\"\"\n",
    "    Positive Pointwise Mutual Information (Church & Hanks, 1990)\n",
    "    \n",
    "    PMI is also available also in NLTK:\n",
    "    from nltk.metrics import BigramAssocMeasures\n",
    "    print BigramAssocMeasures.pmi(8, (15828, 4675), 14307668)\n",
    "    \"\"\"\n",
    "    observed = o_11\n",
    "    expected = (r_1*c_1)/n \n",
    "    res = log(observed/expected,2)\n",
    "    return max(0, res)\n",
    "\n",
    "\n",
    "def plmi(o_11, r_1, c_1, n):\n",
    "    \"\"\"\n",
    "    Positive Local Mutual Information, useful for leveraging the \n",
    "    low-frequency bias of the PPMI\n",
    "    \"\"\"\n",
    "    res = o_11 * ppmi(o_11, r_1, c_1, n)\n",
    "    return res\n",
    "\n",
    "\n",
    "def log_likelihood(o_11, r_1, c_1, n):\n",
    "    \"\"\"\n",
    "    Log Likelihood measure (Dunning, 1993) or \"G-squared\" measure \n",
    "\n",
    "    NOTE that when o_ii is zero log_2(o_ii) is negative infinity and python\n",
    "    raises a ValueError. Howere,we're summing up o_ii * log_2(o_ii/e_ii)\n",
    "    and the limit of this factor goes to zero, so that we can simply\n",
    "    ignore those cells where o_ii is zero\n",
    "    \"\"\"\n",
    "    obs_table = [o_11, r_1 - o_11, c_1 - o_11, n - (c_1 + r_1 - o_11)]  # o_11, o_12, o_21, o_22\n",
    "    exp_table = [(r_1 * c_1)/ n,\n",
    "                 (r_1 * (n - c_1))/ n,\n",
    "                 ((n - r_1) * c_1)/ n,\n",
    "                 ((n - r_1) * (n - c_1))/ n]  # e_11, e_12, e_21, e_22\n",
    "    res = 0\n",
    "    for i, obs in enumerate(obs_table):\n",
    "        try:\n",
    "            res += obs * log(obs / exp_table[i])\n",
    "        except ValueError:\n",
    "            continue\n",
    "    res = 2 * res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'Kong-NOUN', u'Hong-NOUN'), 17.654091187664235), ((u'Hong-NOUN', u'Kong-NOUN'), 17.654091187664235), ((u'Income-NOUN', u'Gross-ADJ'), 17.654091187664235), ((u'Income-NOUN', u'Adjusted-VERB'), 17.376557212135328), ((u'Nam-NOUN', u'Viet-NOUN'), 17.113522806301532), ((u'Viet-NOUN', u'Nam-NOUN'), 17.113522806301532), ((u'Purdew-NOUN', u'Simms-NOUN'), 17.026059965051196), ((u'Simms-NOUN', u'Purdew-NOUN'), 17.026059965051196), ((u'Lao-NOUN', u'Pathet-NOUN'), 17.026059965051196), ((u'lagoon-NOUN', u'aerate-VERB'), 16.91712559349803), ((u'WTV-NOUN', u'antigen-NOUN'), 16.828120587439287), ((u'antigen-NOUN', u'WTV-NOUN'), 16.828120587439287), ((u'Islands-NOUN', u'Guam-NOUN'), 16.818066922775362), ((u'Guam-NOUN', u'Islands-NOUN'), 16.818066922775362), ((u'Iliad-NOUN', u'Odyssey-NOUN'), 16.791594711414174), ((u'Market-NOUN', u'Common-ADJ'), 16.791594711414174), ((u'Tribune-NOUN', u'Herald-NOUN'), 16.76100639158075), ((u'Herald-NOUN', u'Tribune-NOUN'), 16.76100639158075), ((u'Shu-NOUN', u'Lo-NOUN'), 16.721205383522776), ((u'tablespoon-NOUN', u'tablespoon-NOUN'), 16.71264337001935)]\n"
     ]
    }
   ],
   "source": [
    "ppmis_surface = Counter()\n",
    "\n",
    "N = sum(cooccs_surface.itervalues())  # note that this is NOT the reduced dictionary\n",
    "\n",
    "for k,v in filtered_cooccs_surface.iteritems():\n",
    "    ppmis_surface[k] = ppmi(v, selected_brown_noun_frequencies[k[0]], filtered_brown_lemmas_frequencies[k[1]], N)\n",
    "    \n",
    "print ppmis_surface.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'States-NOUN', u'United-VERB'), 4787.77840883779), ((u'York-NOUN', u'New-ADJ'), 3621.1147930991538), ((u'year-NOUN', u'ago-ADV'), 1464.8885373164483), ((u'Island-NOUN', u'Rhode-NOUN'), 1258.5533261922783), ((u'Rhode-NOUN', u'Island-NOUN'), 1258.5533261922783), ((u'Mrs.-NOUN', u'Mrs.-NOUN'), 1187.8825846241932), ((u'year-NOUN', u'last-ADJ'), 928.5404042426062), ((u'House-NOUN', u'White-ADJ'), 851.4369612227013), ((u'World-NOUN', u'War-NOUN'), 810.2368093216919), ((u'War-NOUN', u'World-NOUN'), 810.2368093216919), ((u'year-NOUN', u'fiscal-ADJ'), 806.8805607815154), ((u'Corps-NOUN', u'Peace-NOUN'), 764.6181753280484), ((u'Peace-NOUN', u'Corps-NOUN'), 764.6181753280484), ((u'Angeles-NOUN', u'Los-NOUN'), 725.7315808235118), ((u'Los-NOUN', u'Angeles-NOUN'), 725.7315808235118), ((u'time-NOUN', u'same-ADJ'), 716.6845161499051), ((u'school-NOUN', u'high-ADJ'), 684.1104956382288), ((u'place-NOUN', u'take-VERB'), 682.1673150786678), ((u'Nations-NOUN', u'United-VERB'), 602.2264808844225), ((u'Motors-NOUN', u'General-NOUN'), 588.6906906904063)]\n"
     ]
    }
   ],
   "source": [
    "plmis_surface = Counter()\n",
    "\n",
    "N = sum(cooccs_surface.itervalues())  # note that this is NOT the reduced dictionary\n",
    "\n",
    "for k,v in filtered_cooccs_surface.iteritems():\n",
    "    plmis_surface[k] = plmi(v, selected_brown_noun_frequencies[k[0]], filtered_brown_lemmas_frequencies[k[1]], N)\n",
    "    \n",
    "print plmis_surface.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'States-NOUN', u'United-VERB'), 6953.964479037217), ((u'York-NOUN', u'New-ADJ'), 5218.669840636507), ((u'year-NOUN', u'ago-ADV'), 1864.8375330848553), ((u'Island-NOUN', u'Rhode-NOUN'), 1771.6494950804188), ((u'Rhode-NOUN', u'Island-NOUN'), 1771.6494950804188), ((u'Mrs.-NOUN', u'Mrs.-NOUN'), 1465.3982476383976), ((u'House-NOUN', u'White-ADJ'), 1143.7883279065052), ((u'year-NOUN', u'last-ADJ'), 1092.4697574612487), ((u'Angeles-NOUN', u'Los-NOUN'), 1079.7123743764696), ((u'Los-NOUN', u'Angeles-NOUN'), 1079.7123743764696), ((u'World-NOUN', u'War-NOUN'), 1075.82017865477), ((u'War-NOUN', u'World-NOUN'), 1075.8201786547697), ((u'Corps-NOUN', u'Peace-NOUN'), 1063.633984978412), ((u'Peace-NOUN', u'Corps-NOUN'), 1063.633984978412), ((u'year-NOUN', u'fiscal-ADJ'), 1038.9416903080191), ((u'Motors-NOUN', u'General-NOUN'), 822.8840108112654), ((u'General-NOUN', u'Motors-NOUN'), 822.8840108112654), ((u'time-NOUN', u'same-ADJ'), 821.8472343257614), ((u'San-NOUN', u'Francisco-NOUN'), 821.6474914856732), ((u'Francisco-NOUN', u'San-NOUN'), 821.6474914856732)]\n"
     ]
    }
   ],
   "source": [
    "lls_surface = Counter()\n",
    "\n",
    "N = sum(cooccs_surface.itervalues())  # note that this is NOT the reduced dictionary\n",
    "\n",
    "for k,v in filtered_cooccs_surface.iteritems():\n",
    "    lls_surface[k] = log_likelihood(v, selected_brown_noun_frequencies[k[0]], filtered_brown_lemmas_frequencies[k[1]], N)\n",
    "    \n",
    "print lls_surface.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![alt text](images/3dvectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our vectors by populating a DENSE matrix for each of oue type of weighted co-occurrences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemma 2 rows/column index mapppings\n",
    "sorted_vectors = sorted(selected_brown_noun_frequencies)\n",
    "vectors_indices = dict((v,i) for i,v in enumerate(sorted_vectors))\n",
    "contexts_indices = dict((v,i) for i,v in enumerate(sorted(filtered_brown_lemmas_frequencies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the PPMI Weighted dense matrix\n",
    "ppmiMat = np.zeros((len(vectors_indices), len(contexts_indices)))\n",
    "\n",
    "# populate the matrix\n",
    "for pair, weight in ppmis_surface.iteritems():\n",
    "    ppmiMat[vectors_indices[pair[0]]][contexts_indices[pair[1]]] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print ppmiMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the PLMI Weighted dense matrix\n",
    "plmiMat = np.zeros((len(vectors_indices), len(contexts_indices)))\n",
    "\n",
    "# populate the matrix\n",
    "for pair, weight in plmis_surface.iteritems():\n",
    "    plmiMat[vectors_indices[pair[0]]][contexts_indices[pair[1]]] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the LL Weighted dense matrix\n",
    "llMat = np.zeros((len(vectors_indices), len(contexts_indices)))\n",
    "\n",
    "# populate the matrix\n",
    "for pair, weight in lls_surface.iteritems():\n",
    "    llMat[vectors_indices[pair[0]]][contexts_indices[pair[1]]] = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Cosine Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![alt text](images/vectors-similarity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The diverse similarity measures we've mentioned in class can be calculated by using the **distance** measures available in virtually all the above cited toolkits, or can be easily implemented. \n",
    "\n",
    "For instance, the **cosine similarity** between the two Numpy arrays `v_1` and `v_2` can be calculated by using one of the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_1 = np.array([-0.012813841, -0.024518383, -0.002765056,  0.079496744,  0.063928973, 0.476156960, 0.122111977, 0.322930189, \n",
    "                0.400701256,  0.454048860, 0.525526219])\n",
    "\n",
    "v_2 = np.array([0.64175768,  0.54625694,  0.40728261,  0.24819750,  0.09406221, 0.16681692, -0.04211932, -0.07130129, \n",
    "                -0.08182200, -0.08266852,  -0.07215885])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scipy_cosine(v1, v2):\n",
    "    return 1 - scipy.spatial.distance.cosine(v1,v2)\n",
    "\n",
    "def nltk_cosine(v1, v2):\n",
    "    return 1 - nltk.cluster.cosine_distance(v1,v2)\n",
    "\n",
    "def numpy_cosine(v1, v2):\n",
    "    mag_v1 = np.linalg.norm(v1)\n",
    "    mag_v2 = np.linalg.norm(v2)\n",
    "    return np.dot(v1, v2) / ( mag_v1 * mag_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit scipy_cosine(v_1, v_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit nltk_cosine(v_1, v_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit numpy_cosine(v_1, v_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If (for some reason) you're working on lists, maybe the best solution is to rely on the `math` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def math_cosine(l1, l2):\n",
    "    d = sum(p*q for p,q in zip(l1, l2))\n",
    "    mag_l1 = math.sqrt(sum([n**2 for n in l1]))\n",
    "    mag_l2 = math.sqrt(sum([n**2 for n in l2]))\n",
    "    return d/( mag_l1 * mag_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit math_cosine(v_1.tolist(), v_2.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the calculation of the similarity is (together with the extraction of the co-occurrences) the **main bottleneck** of these models, and the above mentioned solutions become **impractical** when comparing all the vectors in a reasonably sized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to calculate the similarity on the **whole matrix** at once, a viable solution is to use the methods available in the `sklearn.metrics.pairwise` module, among which the `cosine_similarity` one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppmiSimMat = sklearn.metrics.pairwise.cosine_similarity(ppmiMat)\n",
    "plmiSimMat = sklearn.metrics.pairwise.cosine_similarity(plmiMat)\n",
    "llSimMat = sklearn.metrics.pairwise.cosine_similarity(llMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's print the 20 most similar nouns to \"year\" according to each of our spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_idx = vectors_indices[\"year-NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"according to the ppmi space:\\n\"\n",
    "\n",
    "for idx in ppmiSimMat[target_idx,].argsort()[::-1][1:21]:\n",
    "    print sorted_vectors[idx], ppmiSimMat[target_idx, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"according to the plmi space:\\n\"\n",
    "\n",
    "for idx in plmiSimMat[target_idx,].argsort()[::-1][1:21]:\n",
    "    print sorted_vectors[idx], plmiSimMat[target_idx, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"according to the log-likelihood space:\\n\"\n",
    "\n",
    "for idx in llSimMat[target_idx,].argsort()[::-1][1:21]:\n",
    "    print sorted_vectors[idx], llSimMat[target_idx, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 25px 10px 20px\">\n",
    "    <img src=\"images/your_turn.jpg\" width=\"110\">\n",
    "</div>\n",
    "\n",
    "#### Your Turn.\n",
    "\n",
    "\n",
    "Choose three (frequent) nouns and print their top-similar nouns according to each one of the above implemented association measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is convenient to look at the distances between your word in order to have an idea of what's going on with your data. Unfortunately, human being do not cope well with more than 3 dimensions (the novel [Flatland: A Romance of Many Dimensions](https://en.wikipedia.org/wiki/Flatland) may be an insightful reading on this topic). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A usual solution is to reduce the dimensionality of your dataset to **2 or 3 dimensions** trying to preserve the pairwise distances as much as possible. Among the several **non-linear dimensionality reduction techniques** to date available (a quick review is available In the [scikit-learn documentation](http://scikit-learn.org/stable/modules/manifold.html)), the **MULTIDIMENSIONAL SCALING** is widely used to display the information contained in a distance matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use MDS to plot the distance between an (arbitrarily chosen) subset of our nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmas2plot = selected_brown_noun_frequencies.most_common()[200:225]\n",
    "lemmas2plot_idxs = [vectors_indices[lem[0]] for lem in lemmas2plot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measure2mat = {\"ppmi\": ppmiMat, \"plmi\": plmiMat, \"loglikelihood\": llMat}\n",
    "\n",
    "labels = [p[0].split(\"-\")[0] for p in lemmas2plot]\n",
    "\n",
    "for i, (m, mat) in enumerate(measure2mat.items()):\n",
    "    # we need a DISTANCE matrix\n",
    "    distMat = sklearn.metrics.pairwise.pairwise_distances(mat[lemmas2plot_idxs,:], metric=\"cosine\")\n",
    "    \n",
    "    # let's reduce the dimensionality\n",
    "    mds = sklearn.manifold.MDS(n_components = 2, max_iter = 300, eps = 1e-9, \n",
    "                               random_state = np.random.RandomState(seed = 6), \n",
    "                               dissimilarity = \"precomputed\")\n",
    "\n",
    "    coords = mds.fit(distMat).embedding_\n",
    "    \n",
    "    # let's plot the distances\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], marker = '.', c = 'r')\n",
    "    \n",
    "    \n",
    "    \n",
    "    for label, x, y in zip(labels, coords[:, 0], coords[:, 1]):\n",
    "        plt.annotate(label, xy = (x,y), horizontalalignment = 'center', verticalalignment='bottom')\n",
    "\n",
    "    plt.title(\"MDS on the \" + m +\"-based space\")\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another viable option is to perform use the pairwise similarities in order to organize our words into groups (i.e. to performs **clustering**). The general idea is to organize clusters so as to maximize the within-group similiarities and the between-group differences. There are many clustering algorithms and techniques, all of them with their pros and cons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierachical clustering** techniques do not try to produce a single clustering, but try to produce a hierarchy of groups. As such, they are quite useful in exploratory analysis. In this class, the **Ward** algorithm is an example of **agglomerative** clustering:  each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. In the Ward's method, the distance between two clusters is the change in the sum of squared distances when they are merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `scipy.cluster.hierarchy.ward()` function to create our hierarchical clustering and the  `scipy.cluster.hierarchy.dendrogram()` function to visualize it (i.e. to create a dendrogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measure2mat = {\"ppmi\": ppmiMat, \"plmi\": plmiMat, \"loglikelihood\": llMat}\n",
    "\n",
    "labels = [p[0].split(\"-\")[0] for p in lemmas2plot]\n",
    "\n",
    "for i, (m, mat) in enumerate(measure2mat.items()):\n",
    "    \n",
    "    # let's built a DISTANCE matrix\n",
    "    distMat = sklearn.metrics.pairwise.pairwise_distances(mat[lemmas2plot_idxs,:], metric=\"cosine\")\n",
    "    \n",
    "    # as we'll rely the scikit learn function to perform clustering, it is safer to convert our redundant distance matrix \n",
    "    # into a condensed distance matrix (i.e. into a flat array containing the upper triangular of the distance matrix)\n",
    "    condensed_distMat = scipy.spatial.distance.squareform(distMat)\n",
    "    \n",
    "    # we could alternatively have used scipy.spatial.distance.pdist() to obtain the distance matric in this format\n",
    "    # scipyPdist = scipy.spatial.distance.pdist(mat[lemmas2plot_idxs,:], \"cosine\")\n",
    "    \n",
    "    # let's perform the hierarchical clustering\n",
    "    linkage_matrix = scipy.cluster.hierarchy.ward(condensed_distMat)\n",
    "    \n",
    "    # let's create the dendrogram (i.e. let's visualize the structure of the groups)\n",
    "    scipy.cluster.hierarchy.dendrogram(linkage_matrix, orientation=\"right\", labels=labels)\n",
    "    \n",
    "    plt.xlabel (\"distance (not cosines, but those used by the Ward's method)\")\n",
    "\n",
    "    plt.title(\"MDS on the \" + m +\"-based space\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VSMs can be evaluated on **Semantic Similarity Tasks**, in which they are required to simulate the pairwise similarity judgments collected from speakers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we evaluate our models against the [WordSim-353 dataset](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/):\n",
    "\n",
    "\n",
    "- 353 noun pairs rated on a 0-10 scale \n",
    "\n",
    "\n",
    "- measure: spearman's correlation between algorithm and human word similarity ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's select the pairs of nouns for which we have aenough disitrbutional information\n",
    "\n",
    "wordSim353 = dict()\n",
    "\n",
    "with open(\"data/wordSim353.csv\",\"rb\") as infile:\n",
    "    infile.next()\n",
    "    for line in infile:\n",
    "        raw_w1, raw_w2, rating = unicode(line).strip().split(\",\")\n",
    "        w1 = raw_w1+\"-NOUN\"\n",
    "        w2 = raw_w2+\"-NOUN\"\n",
    "        if all([vectors_indices.has_key(w) for w in [w1,w2]]):\n",
    "            wordSim353[(w1, w2)] = float(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that due to the size of our corpus we've lost 1/3 of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(wordSim353) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measure2mat = {\"positivePMI\": ppmiSimMat, \"positiveLMI\": plmiSimMat, \"log-likelihood\": llSimMat}\n",
    "\n",
    "rhos = []\n",
    "measures = []\n",
    "\n",
    "for m, mat in measure2mat.items():\n",
    "    print m+\"-based space vs. wordSim353 -> spearman's rho:\\t\", \n",
    "    \n",
    "    wordSim_ratings = []\n",
    "    vsm_sims = []\n",
    "    for (w1, w2), r in wordSim353.iteritems():\n",
    "        w1idx = vectors_indices[w1]\n",
    "        w2idx = vectors_indices[w2]\n",
    "        \n",
    "        wordSim_ratings.append(r)\n",
    "        vsm_sims.append(mat[w1idx, w2idx])\n",
    "        \n",
    "    rho, pval = scipy.stats.spearmanr(wordSim_ratings, vsm_sims)\n",
    "    \n",
    "    print rho\n",
    "    rhos.append(rho)\n",
    "    measures.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.bar(range(0, len(rhos)), rhos)\n",
    "plt.xticks(range(0, len(rhos)), measures, size='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dense Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Up to this moment, we've worked with **long** and **sparse** vectors:\n",
    "\n",
    "\n",
    "- the proportion of zeros in our ppmi weighted matrix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(ppmiMat) / ppmiMat.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the length of our vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppmiMat.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Denser** representations have a number of **advantages**, among which:\n",
    "\n",
    "- their computationally easier to manipulate (e.g. to create  or to use as ML features)\n",
    "\n",
    "\n",
    "- they tend to be less noisy\n",
    "\n",
    "\n",
    "- they exploit latent meaning dimensions (e.g. they may tend to merge together correlated contexts like *car* and *automobile*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dimensionality Reduction**: the process of reducing the number of features (i.e. contextual markers) by mapping data points (i.e. the vectors describing the target words) into a **low-dimensional subspace**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction via Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *The SVD is the Swiss Army knife of matrix decompositions* (O'Leary, 2006)\n",
    "\n",
    "\n",
    "- SVD is a method for finding, for a given dataset, the **dimensions on which the data varies the most** (Jurafsky & Martin, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuitive Idea**: rotate the axes of the dataset into a new space, so that the order of the dimension is related to the quantity of captured variance\n",
    "\n",
    "- first dimension capture the most variance, then the second captures most of the variance unexplained by the first dimension and so on...\n",
    "\n",
    "\n",
    "- many related methods: PCA, Factor Analysis, **SVD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorization\n",
    "\n",
    "SVD **factorizes** a (non-symmetric, non-square) $w \\times c$ matrix $M$ into the **product of three matrices**: $U \\cdot\\Sigma \\cdot V^T$, where:\n",
    "\n",
    "\n",
    "- $U$: the rows correspond to the rows of M (i.e. our target words) and the columns represent the dimensions of the **latent space**\n",
    "    - dimensions are orthogonal\n",
    "    - dimensions are sorted according to the amount of variance they explain\n",
    "    - e.g. the word frequency distribution for any latent topic\n",
    "    \n",
    "    \n",
    "- $\\Sigma$: a diagonal $m \\times m$ matrix (where $m$ is the rank of our matrix $M$) of singular values in decreasing order\n",
    "    - intuitively, the **square** each singular value $\\sigma$ express the importance of each dimension\n",
    "    - e.g. the prior frequency of each latent topic\n",
    "\n",
    "    \n",
    "- $V^T$: the columns correspond the columns of M and the rows correspond to the singular values \n",
    "    - e.g. the contribution of each latent topic to each context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathop{\\begin{bmatrix}\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & M & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    "\\end{bmatrix}}_{\\textstyle w \\times c} \\ = \\\n",
    "\\mathop{\\begin{bmatrix}\n",
    " & & & & \\\\\n",
    " & & & & \\\\\n",
    " & & & & \\\\\n",
    " & & & & \\\\\n",
    " & & U & & \\\\\n",
    " & & & & \\\\\n",
    " & & & & \\\\\n",
    " & & & & \\\\\n",
    " & & & & \\\\\n",
    "\\end{bmatrix}}_{\\textstyle w \\times m}\\ \n",
    "\\mathop{\\begin{bmatrix}\n",
    "\\sigma_1 & 0 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\sigma_2 & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & \\sigma_3 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & \\sigma_m \\\\\n",
    "\\end{bmatrix}}_{\\textstyle m \\times m}\\ \n",
    "\\mathop{\\begin{bmatrix}\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & V^T & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    "\\end{bmatrix}}_{\\textstyle m \\times c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncation\n",
    "\n",
    "If we use the **first $d$ singular values**, we obtain a **truncated matrix** $\\tilde{M}_d$ that is **a least-squares approximation** to the original $M$\n",
    "\n",
    "\n",
    "- dimensions $d < x < m$ are descarded\n",
    "    - by removing the dimensions that encode the least variance, we **remove noise**\n",
    "\n",
    "\n",
    "- generally $50 < d < 1000$ (with a soft spot around 300)\n",
    "\n",
    "\n",
    "- some authors (Lapesa & Evert, 2014) suggest that for some task it may be useful to get rid of the very first dimensions as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathop{\\begin{bmatrix}\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & \\tilde{M}_d & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    " & & & & & &\\\\\n",
    "\\end{bmatrix}}_{\\textstyle w \\times c} \\ = \\\n",
    "\\mathop{\\begin{bmatrix}\n",
    " & & \\\\\n",
    " & & \\\\\n",
    " & & \\\\\n",
    " & & \\\\\n",
    " & U_d &\\\\\n",
    " & & \\\\\n",
    " & & \\\\\n",
    " & & \\\\\n",
    " & & \\\\\n",
    "\\end{bmatrix}}_{\\textstyle w \\times d}\\ \n",
    "\\mathop{\\begin{bmatrix}\n",
    " & &\\\\\n",
    " & \\Sigma_d & \\\\\n",
    " & & \\\\\n",
    "\\end{bmatrix}}_{\\textstyle d \\times d}\\\n",
    "\\mathop{\\begin{bmatrix}\n",
    " & & & & & &\\\\\n",
    " & & & V^T_d & & &\\\\\n",
    " & & & & & &\\\\\n",
    "\\end{bmatrix}}_{\\textstyle d \\times c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "\n",
    "We can take advantage of the fact that **dot-products between** the rows of $W = U_d \\cdot \\Sigma_d$ are equal to those of the rows of $\\tilde{M}_d = U_d \\cdot \\Sigma_d\\cdot V^T_d$, so that we can **ignore $V^T_d$** and take the rows of $W$ as our word representations.\n",
    "\n",
    "Recently, it has been showed (but not explained) that an additional parameter $p$ should be added to control the eigenvalue matrix $\\Sigma$, so that we will take as **word representations** the rows of the $W'$ matrix, defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W' = U_d \\cdot \\Sigma_d^p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note that for some tasks, it has been suggested that ignoring $\\Sigma$ alltogether (i.e. setting $p = 0$) may lead to an increase in performance\n",
    "\n",
    "\n",
    "- some authors (e.g. Jurafsky & Martin, 2018) use this setting as the default, but it is wiser to spend some time in tuning this hyperparameters rather than ignoring it altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 1: **using Scikit-learn**\n",
    "\n",
    "\n",
    "- you cannot set $p$ (it's always $p = 1$)\n",
    "\n",
    "\n",
    "- you have to know how many dimensions you want in advance\n",
    "\n",
    "\n",
    "- you cannot descard the first dimensions, if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note that the default SVD solver is the randomized algorithm proposed by Halko (2009), but we select the ARPACK wrapper \n",
    "# in SciPy (scipy.sparse.linalg.svds) because is more efficient (but slower)\n",
    "ppmiMat_tr300 = sklearn.decomposition.TruncatedSVD(n_components = dimensions, algorithm = \"arpack\").fit_transform(ppmiMat)\n",
    "\n",
    "ppmiSVD300SimMat = sklearn.metrics.pairwise.cosine_similarity(ppmiMat_tr300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measure2mat = {\"ppmi_SVD300\": ppmiSVD300SimMat}\n",
    "\n",
    "for m, mat in measure2mat.items():\n",
    "    print m+\"-based space vs. wordSim353 -> spearman's rho:\\t\", \n",
    "    \n",
    "    wordSim_ratings = []\n",
    "    vsm_sims = []\n",
    "    for (w1, w2), r in wordSim353.iteritems():\n",
    "        w1idx = vectors_indices[w1]\n",
    "        w2idx = vectors_indices[w2]\n",
    "        \n",
    "        wordSim_ratings.append(r)\n",
    "        vsm_sims.append(mat[w1idx, w2idx])\n",
    "        \n",
    "    rho, pval = scipy.stats.spearmanr(wordSim_ratings, vsm_sims)\n",
    "    \n",
    "    print rho\n",
    "    rhos.append(rho)\n",
    "    measures.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.bar(range(0, len(rhos)), rhos)\n",
    "plt.xticks(range(0, len(rhos)), measures, size='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it worked! But let's see what happens if we play a bit with our hyperparamaters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 2: **using Scipy modules**\n",
    "\n",
    "We can (roughly) re-implement what we've done with scikit-learn as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import linalg, dot\n",
    "\n",
    "# let's decompose our matrix (we can safely ignore Vt)\n",
    "U, s, Vt = linalg.svd(ppmiMat)  \n",
    "\n",
    "# SVD suffers from a problem called \"sign indeterminancy\", so that the sign of the components (i.e. U and Vt) depends\n",
    "# on the initial state and on the algorithm. As a consequence, if U (and V) are mostly negative, their sign should be inverted\n",
    "if not U[U>0].size > (U.size / 2):\n",
    "    U = -U\n",
    "    Vt = -Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's filter out all dimensions but the first 300\n",
    "W = U[:, 0 : dimensions]\n",
    "sigma = s[:dimensions]  # note that for efficiency reasons sigma is an array, not a matrix\n",
    "C = Vt[0 : dimensions, :]\n",
    "\n",
    "ppmiMat_tr300_alt = dot(W, linalg.diagsvd(sigma, dimensions, len(C)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLAINED VARIANCE: the **Scree Plot**\n",
    "\n",
    "A graph plotting variances against singular values is called a scree plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose how many dimensions to retain by checking the portion of variance each dimension is able to explain:\n",
    "\n",
    "- given that the square of each singular value is proportional to the variance explained by each singular vector, we compute the relative contribution of each singular value by dividing the square of the singular value $k$ by the sum of the squares of all the singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel_variance = ( s**2 / sum(s**2) ) * 100\n",
    "y_values =  rel_variance\n",
    "x_values = np.arange(len(y_values))\n",
    "\n",
    "plt.plot(x_values, y_values, linestyle = '-')\n",
    "\n",
    "plt.xlabel(\"Singular Values\", labelpad = 20)\n",
    "plt.ylabel(\"Relative Variance (%)\", labelpad = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the first singular values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_values =  rel_variance[:500] \n",
    "x_values = np.arange(len(y_values))\n",
    "\n",
    "plt.plot(x_values, y_values, linestyle = '-')\n",
    "\n",
    "plt.xlabel(\"Singular Values\", labelpad = 20)\n",
    "plt.ylabel(\"Relative Variance (%)\", labelpad = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = 50\n",
    "\n",
    "W = U[:, 0 : dimensions]\n",
    "sigma = s[:dimensions]\n",
    "C = Vt[0 : dimensions, :]\n",
    "\n",
    "ppmiMat_tr50 = dot(W, linalg.diagsvd(sigma, dimensions, len(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measure2mat = {\"ppmi_SVD50\": ppmiMat_tr50}\n",
    "\n",
    "for m, mat in measure2mat.items():\n",
    "    print m+\"-based space vs. wordSim353 -> spearman's rho:\\t\", \n",
    "    \n",
    "    sim = sklearn.metrics.pairwise.cosine_similarity(mat)\n",
    "    \n",
    "    wordSim_ratings = []\n",
    "    vsm_sims = []\n",
    "    for (w1, w2), r in wordSim353.iteritems():\n",
    "        w1idx = vectors_indices[w1]\n",
    "        w2idx = vectors_indices[w2]\n",
    "        \n",
    "        wordSim_ratings.append(r)\n",
    "        vsm_sims.append(sim[w1idx, w2idx])\n",
    "        \n",
    "    rho, pval = scipy.stats.spearmanr(wordSim_ratings, vsm_sims)\n",
    "    \n",
    "    print rho\n",
    "    rhos.append(rho)\n",
    "    measures.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.bar(range(0, len(rhos)), rhos)\n",
    "plt.xticks(range(0, len(rhos)), measures, size='small', rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 25px 10px 20px\">\n",
    "    <img src=\"images/your_turn.jpg\" width=\"110\">\n",
    "</div>\n",
    "\n",
    "#### Your Turn.\n",
    "\n",
    "Try to remove the first dimensions as well. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEIGHTING $\\Sigma$\n",
    "\n",
    "Some authors suggested to downgrade the influence of $\\Sigma$ on the final word representation by applying a weight $0 < p < 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's use only U, i.e. if p == 0\n",
    "\n",
    "p = 0.\n",
    "\n",
    "W = U[:, 0 : dimensions]\n",
    "sigma = s[:dimensions]\n",
    "C = Vt[0 : dimensions, :] \n",
    "\n",
    "ppmiMat_tr50_p0 = dot(W, linalg.diagsvd(sigma ** p , dimensions, len(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is equivalent to using only W\n",
    "np.allclose(ppmiMat_tr50_p0, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "\n",
    "W = U[:, 0 : dimensions]\n",
    "sigma = s[:dimensions]\n",
    "C = Vt[0 : dimensions, :] \n",
    "\n",
    "ppmiMat_tr50_p5 = dot(W, linalg.diagsvd(sigma ** p, dimensions, len(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measure2mat = {\"ppmi_SVD50_p.0\": ppmiMat_tr50_p0,\n",
    "              \"ppmi_SVD50_p.5\": ppmiMat_tr50_p5,}\n",
    "\n",
    "for m, mat in measure2mat.items():\n",
    "    print m+\"-based space vs. wordSim353 -> spearman's rho:\\t\", \n",
    "    \n",
    "    sim = sklearn.metrics.pairwise.cosine_similarity(mat)\n",
    "    \n",
    "    wordSim_ratings = []\n",
    "    vsm_sims = []\n",
    "    for (w1, w2), r in wordSim353.iteritems():\n",
    "        w1idx = vectors_indices[w1]\n",
    "        w2idx = vectors_indices[w2]\n",
    "        \n",
    "        wordSim_ratings.append(r)\n",
    "        vsm_sims.append(sim[w1idx, w2idx])\n",
    "        \n",
    "    rho, pval = scipy.stats.spearmanr(wordSim_ratings, vsm_sims)\n",
    "    \n",
    "    print rho\n",
    "    rhos.append(rho)\n",
    "    measures.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.bar(range(0, len(rhos)), rhos)\n",
    "plt.xticks(range(0, len(rhos)), measures, size='small', rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "\n",
    "Apply SVD to the PLMI Weighted dense matrix `plmiMat` and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.\n",
    "\n",
    "Create and evaluate a plmi-weighted DSM based on the co-occurrences in a ±10 words collocational span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "\n",
    "create a plmi-based space build from the following **sentence splitted and tokenized** version of the Brown corpus obtained by:\n",
    "\n",
    "- filtering out the tokens composed solely by punctuation marks and\n",
    "\n",
    "- lowering all the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "brown_words_raw = []\n",
    "\n",
    "for sentence in nltk.corpus.brown.sents():\n",
    "    brown_words_raw.append([w.lower() for w in sentence if not set(w).issubset(string.punctuation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ignore what follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export this notebook as a HTML file\n",
    "# !jupyter nbconvert TMCI-2017-w7a --output html_converted_notebooks/TMCI-2017-w7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
