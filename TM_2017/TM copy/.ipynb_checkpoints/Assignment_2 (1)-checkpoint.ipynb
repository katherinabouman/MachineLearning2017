{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:5px 10px 5px 10px\" markdown=\"1\">\n",
    "    <img src=\"images/auc.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right;margin-top:10px\" markdown=\"1\">\n",
    "    <h3><i>Text Mining & Collective Intelligence</i></h3>\n",
    "</div>\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "<center><h3>Assignment #2</h3>\n",
    "\n",
    "<h2>Foundations of NLP</h2>\n",
    "\n",
    "<br>\n",
    "<h3>• deadline: 03/11/2017 •</h3>\n",
    "\n",
    "</center>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student's name: Katja Bouman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student's ID: 11155787"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### By submitting this notebook, you implicitly agree to the following code of conduct:\n",
    "\n",
    ">My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    ">\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    ">\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<div style=\"float:right;margin: 10px 0 0 0 \" markdown=\"1\">\n",
    "    (from Coursera's [Honor Code](https://www.coursera.org/about/terms/honorcode))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 1. (10 points)\n",
    "\n",
    "Use the [Stanford PoS-tagger](https://nlp.stanford.edu/software/tagger.html#Download) to automatically identify the morphological class of the words in the `nltk.corpus.reuters` corpus.\n",
    "\n",
    "\n",
    "It is quite a long process, that can be made faster by:\n",
    "\n",
    "\n",
    "- using the function `.tag_sents()` rather than `.tag()` to annotate entire sentences. You may run into an OOM error, for which possible solutions may be:\n",
    "    - to change the Java heap space with the following command `nltk.internals.config_java(options='-Xmx8064m')`, depending on your RAM\n",
    "    \n",
    "    - to use the lighter `english-left3words-distsim.tagger` model in place of the slighlty more accurate `english-bidirectional-distsim.tagger` model we've used in class\n",
    "    \n",
    "    - to annotate just few corpus documents (e.g. 1000) per run\n",
    "\n",
    "\n",
    "\n",
    "- using the `dump()` and `load()` functions from the [cPickle](https://docs.python.org/2/library/pickle.html) module to, respectively, store your corpus in a txt file and to load it for the next exercises.\n",
    "\n",
    "    - **IMPORTANT**: please, submit this external file (in a .zip archive) together with your assignment. Failing to comply to this requirement will result in a 10 points penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import math \n",
    "\n",
    "from itertools import chain, permutations, product\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import cluster\n",
    "from sklearn import manifold, metrics\n",
    "\n",
    "import cPickle\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_postagger = nltk.tag.stanford.StanfordPOSTagger(\n",
    "    './stanford-postagger-2017-06-09/models/english-bidirectional-distsim.tagger', \n",
    "    './stanford-postagger-2017-06-09/stanford-postagger.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To prevent the java error:\n",
    "import os\n",
    "java_path = \"C:/Program Files (x86)/Java/jre1.8.0_91/bin/java.exe\"\n",
    "#Can't find my own path ...\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-b510afbd0519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mPoS_reuters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menglish_postagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"PoS_file.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/nltk/tag/stanford.pyc\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Run the tagger and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n\u001b[0;32m---> 99\u001b[0;31m                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# Check the return code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_poll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_with_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_with_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate_with_poll\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mfd2file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#First tokenize the corpus\n",
    "text = nltk.corpus.reuters.sents()\n",
    "\n",
    "#Then use the Stanford Post-tagger to identify the morphological classes of the words and put them in a file using Pickle\n",
    "#because it is a very large corpus\n",
    "PoS_reuters = english_postagger.tag_sents(text)\n",
    "\n",
    "with open(r\"PoS_file.pickle\", \"wb\") as output_file:\n",
    "    cPickle.dump(PoS_reuters, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. (20 points)\n",
    "\n",
    "Create two distributional spaces by **counting and filtering** the surface co-occurrences in a symmetric ±5w collocations span from the following corpora:\n",
    "\n",
    "\n",
    "- a stemmed version of the reuters corpus (the choice of the actual stemmer is up to you)\n",
    "\n",
    "\n",
    "- a lemmatized version of the reuter corpus that you should have annotated with the Stanford PoS-tagger. In the case you couldn't perform the previous esercise, please use the following command to load the corpus stored in `data/reuters.pos` \n",
    "\n",
    "```python\n",
    "with open(\"data/reuters.pos\", \"rb\") as corpus_file:\n",
    "    reuter_PoSTagged = cPickle.load(corpus_file)\n",
    "```\n",
    "\n",
    "NB: please remember that the tagset used by the Spanford PoS-tagger in the one proposed for the [Penn Treebank](Penn Treebank site), see also `nltk.help.upenn_tagset()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, state explicitely:\n",
    "\n",
    "- what lemmas do you want to describe (i.e. what will be your vectors?)\n",
    "\n",
    "\n",
    "- how do you want to describe them (i.e. what will be your contexts?)\n",
    "\n",
    "\n",
    "- what filtering strategy are you going to choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/reuters.pos\", \"rb\") as corpus_file:\n",
    "    reuter_PoSTagged = cPickle.load(corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemmed version\n",
    "stemmed_text = []\n",
    "\n",
    "#Use the Porter Stemmer on the PoSTagged reuters corpus\n",
    "for i in range(len(reuter_PoSTagged)):\n",
    "    stemmed_sentence = []\n",
    "    for w, p in reuter_PoSTagged[i]:\n",
    "        if p in [\".\", \"X\"]:\n",
    "                continue\n",
    "        else:\n",
    "            stem = nltk.PorterStemmer().stem(w)\n",
    "        stemmed_sentence.append(\"-\".join([stem, p]))\n",
    "        i += 1    \n",
    "            \n",
    "    stemmed_text.append(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Counting\n",
    "#First find permutations\n",
    "cooccs_Stextual = Counter()\n",
    "\n",
    "for sentence in stemmed_text:\n",
    "    cooccs_Stextual.update([pair for pair in set(permutations(sentence, 2)) if pair[0].split(\"-\")[-1] == \"NN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find cocurencies in span size 5 words on each side of the word\n",
    "spansize = 5\n",
    "cooccs_surface = Counter()\n",
    "\n",
    "for sentence in stemmed_text:\n",
    "    for i,w in enumerate(sentence):\n",
    "        if w.split(\"-\")[-1] == \"NN\":\n",
    "            span_range = range(max(i- spansize, 0), i)  # 5 words on the left \n",
    "            span_range.extend(range(i+1, min(i + spansize + 1, len(sentence))))  # and 5 words on the right\n",
    "            for cw in [sentence[idx] for idx in span_range]:\n",
    "                cooccs_surface[(w, cw)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a count of the frequencies of the lemmas and the nouns\n",
    "stemmed_frequencies = Counter(chain(*stemmed_text))\n",
    "stemmed_noun_frequencies = Counter([w for w in chain(*stemmed_text) if w.split(\"-\")[-1] == \"NN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2581 context lemmas\n",
      "[(u'lt-NN', 8592), (u'year-NN', 7436), (u'S-NN', 6045), (u'loss-NN', 4695), (u'compani-NN', 4499), (u'share-NN', 3068), (u'trade-NN', 3021), (u'profit-NN', 2901), (u'market-NN', 2771), (u'oil-NN', 2583), (u'stock-NN', 2390), (u'shr-NN', 2277), (u'qtr-NN', 1996), (u'quarter-NN', 1787), (u'price-NN', 1781), (u'product-NN', 1769), (u'rate-NN', 1717), (u'bank-NN', 1716), (u'govern-NN', 1612), (u'week-NN', 1559), (u'dollar-NN', 1523), (u'offer-NN', 1491), (u'tax-NN', 1472), (u'today-NN', 1407), (u'interest-NN', 1371), (u'agreement-NN', 1360), (u'month-NN', 1267), (u'group-NN', 1240), (u'dlr-NN', 1239), (u'net-NN', 1229), (u'growth-NN', 1154), (u'stg-NN', 1106), (u'exchang-NN', 1072), (u'end-NN', 1034), (u',\"-NN', 1018), (u'meet-NN', 992), (u'.\"-NN', 989), (u'spokesman-NN', 957), (u'cash-NN', 951), (u'avg-NN', 938), (u'sale-NN', 932), (u'industri-NN', 891), (u'wheat-NN', 888), (u'world-NN', 888), (u'board-NN', 882), (u'export-NN', 874), (u'busi-NN', 846), (u'money-NN', 844), (u'time-NN', 831), (u'buy-VB', 828), (u'invest-NN', 828), (u'debt-NN', 819), (u'rise-NN', 802), (u'deficit-NN', 802), (u'period-NN', 794), (u'merger-NN', 791), (u'oper-NN', 786), (u'statement-NN', 785), (u'stake-NN', 782), (u'dividend-NN', 772)]\n"
     ]
    }
   ],
   "source": [
    "#Filtering\n",
    "filtered_stemmed_frequencies = Counter()\n",
    "\n",
    "#First exclude the closed class words, and the words that are in the specified list\n",
    "#The specific words in the list are words that do not give any information about the text\n",
    "#Either because they are verbs like to be or to have, the words: mln,  pct, Shr, QTR were very frequent though do not mean anything\n",
    "\n",
    "for k,v in stemmed_frequencies.iteritems():\n",
    "    if v >= 10 and \\\n",
    "    k.split(\"-\")[-1] in [\"NN\", \"VB\", \"VBG\", \"ADJ\", \"ADV\"]\\\n",
    "    and k.split(\"-\")[0] not in [\"be\", \"have\", \"do\", \"would\", \"will\", \"could\", \"mln\", \"pct\", \"Shr\", \"QTR\"]:\n",
    "        filtered_stemmed_frequencies[k] = v\n",
    "\n",
    "print len(filtered_stemmed_frequencies), \"context lemmas\"\n",
    "print filtered_stemmed_frequencies.most_common(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({u'compani-NN': 4499, u'share-NN': 3068, u'trade-NN': 3021, u'profit-NN': 2901, u'market-NN': 2771, u'stock-NN': 2390, u'quarter-NN': 1787, u'price-NN': 1781, u'product-NN': 1769, u'govern-NN': 1612, u'dollar-NN': 1523, u'offer-NN': 1491, u'today-NN': 1407, u'interest-NN': 1371, u'agreement-NN': 1360, u'month-NN': 1267, u'group-NN': 1240, u'growth-NN': 1154, u'exchang-NN': 1072, u'spokesman-NN': 957, u'industri-NN': 891, u'wheat-NN': 888, u'world-NN': 888, u'board-NN': 882, u'export-NN': 874, u'money-NN': 844, u'invest-NN': 828, u'deficit-NN': 802, u'period-NN': 794, u'merger-NN': 791, u'statement-NN': 785, u'stake-NN': 782, u'dividend-NN': 772, u'increas-NN': 733, u'offici-NN': 721, u'record-NN': 718, u'report-NN': 712, u'currenc-NN': 694, u'sugar-NN': 681, u'yesterday-NN': 679, u'surplu-NN': 671, u'acquisit-NN': 663, u'polici-NN': 656, u'demand-NN': 655, u'manag-NN': 647, u'tender-NN': 634, u'output-NN': 619, u'capit-NN': 615, u'subsidiari-NN': 606, u'economi-NN': 590, u'level-NN': 574, u'credit-NN': 572, u'chairman-NN': 562, u'incom-NN': 562, u'system-NN': 552, u'grain-NN': 539, u'state-NN': 536, u'program-NN': 522, u'presid-NN': 506, u'countri-NN': 505, u'coffe-NN': 496, u'propos-NN': 490, u'suppli-NN': 466, u'account-NN': 455, u'inflat-NN': 440, u'action-NN': 421, u'budget-NN': 411, u'split-NN': 387, u'intervent-NN': 385, u'takeov-NN': 383, u'consum-NN': 381, u'purchas-NN': 377, u'approv-NN': 377, u'amount-NN': 369, u'accord-NN': 362, u'import-NN': 361, u'point-NN': 358, u'barrel-NN': 358, u'financ-NN': 357, u'sector-NN': 355, u'contract-NN': 352, u'transact-NN': 349, u'number-NN': 348, u'decis-NN': 347, u'depart-NN': 338, u'analyst-NN': 338, u'plant-NN': 336, u'confer-NN': 333, u'balanc-NN': 331, u'develop-NN': 330, u'declin-NN': 325, u'target-NN': 322, u'quota-NN': 319, u'total-NN': 311, u'effect-NN': 305, u'cocoa-NN': 303, u'soybean-NN': 297, u'result-NN': 292, u'buffer-NN': 284, u'posit-NN': 283, u'administr-NN': 280, u'control-NN': 279, u'chang-NN': 279, u'produc-NN': 276, u'committe-NN': 271, u'forecast-NN': 268, u'season-NN': 268, u'index-NN': 267, u'pressur-NN': 266, u'offic-NN': 265, u'charg-NN': 264, u'letter-NN': 264, u'payment-NN': 261, u'member-NN': 258, u'respons-NN': 250, u'union-NN': 248, u'turnov-NN': 246, u'capac-NN': 244, u'energi-NN': 239, u'impact-NN': 238, u'agenc-NN': 235, u'explor-NN': 235, u'liquid-NN': 233, u'divis-NN': 233, u'stabil-NN': 231, u'equiti-NN': 230, u'figur-NN': 230, u'tomorrow-NN': 227, u'addit-NN': 227, u'sharehold-NN': 224, u'support-NN': 221, u'reduct-NN': 220, u'restructur-NN': 219, u'distribut-NN': 217, u'spend-NN': 215, u'situat-NN': 215, u'equip-NN': 214, u'deliveri-NN': 213, u'discount-NN': 212, u'order-NN': 212, u'investor-NN': 211, u'servic-NN': 210, u'strike-NN': 209, u'competit-NN': 207, u'provis-NN': 207, u'copper-NN': 206, u'process-NN': 203, u'averag-NN': 200, u'comment-NN': 196, u'steel-NN': 195, u'concern-NN': 195, u'cotton-NN': 195, u'field-NN': 191, u'intent-NN': 190, u'problem-NN': 190, u'studi-NN': 190, u'consumpt-NN': 190, u'pipelin-NN': 188, u'shipment-NN': 187, u'packag-NN': 186, u'insur-NN': 184, u'option-NN': 184, u'volum-NN': 184, u'ventur-NN': 182, u'reserv-NN': 182, u'ministri-NN': 179, u'estim-NN': 178, u'weather-NN': 177, u'director-NN': 176, u'futur-NN': 175, u'announc-NN': 174, u'execut-NN': 174, u'close-NN': 173, u'comput-NN': 172, u'minist-NN': 170, u'interview-NN': 170, u'winter-NN': 169, u'expans-NN': 168, u'perform-NN': 165, u'court-NN': 164, u'revenu-NN': 164, u'council-NN': 163, u'crude-NN': 162, u'sterl-NN': 160, u'commod-NN': 159, u'place-NN': 157, u'class-NN': 157, u'session-NN': 157, u'pound-NN': 154, u'economist-NN': 154, u'return-NN': 153, u'nation-NN': 153, u'unemploy-NN': 153, u'legisl-NN': 152, u'partnership-NN': 152, u'inform-NN': 151, u'harvest-NN': 151, u'manufactur-NN': 150, u'paper-NN': 150, u'disput-NN': 149, u'refineri-NN': 147, u'power-NN': 145, u'research-NN': 145, u'principl-NN': 144, u'measur-NN': 144, u'properti-NN': 144, u'project-NN': 143, u'improv-NN': 142, u'outlook-NN': 141, u'major-NN': 141, u'sourc-NN': 141, u'agricultur-NN': 140, u'assist-NN': 139, u'reason-NN': 139, u'rubber-NN': 138, u'shortag-NN': 137, u'drill-NN': 136, u'newspap-NN': 136, u'barley-NN': 136, u'activ-NN': 136, u'specul-NN': 135, u'gasolin-NN': 135, u'round-NN': 134, u'petroleum-NN': 130, u'attack-NN': 130, u'secur-NN': 129, u'construct-NN': 129, u'recoveri-NN': 128, u'technolog-NN': 128, u'settlement-NN': 126, u'treasuri-NN': 126, u'associ-NN': 126, u'press-NN': 125, u'effort-NN': 124, u'question-NN': 124, u'right-NN': 124, u'grade-NN': 124, u'start-NN': 123, u'famili-NN': 123, u'retali-NN': 122, u'night-NN': 122, u'qualiti-NN': 122, u'review-NN': 122, u'yield-NN': 122, u'calendar-NN': 121, u'percentag-NN': 121, u'estat-NN': 119, u'dealer-NN': 118, u'televis-NN': 118, u'trend-NN': 118, u'attempt-NN': 118, u'buyout-NN': 118, u'complet-NN': 116, u'silver-NN': 115, u'access-NN': 115, u'progress-NN': 114, u'possibl-NN': 113, u'region-NN': 113, u'failur-NN': 113, u'acreag-NN': 111, u'transport-NN': 110, u'damag-NN': 110, u'institut-NN': 109, u'deposit-NN': 109, u'cooper-NN': 109, u'trust-NN': 108, u'crisi-NN': 107, u'veget-NN': 107, u'request-NN': 106, u'custom-NN': 106, u'parent-NN': 106, u'semiconductor-NN': 105, u'adjust-NN': 104, u'cargo-NN': 102, u'facil-NN': 102, u'health-NN': 101, u'limit-NN': 101, u'weekend-NN': 101, u'reform-NN': 99, u'requir-NN': 98, u'telephon-NN': 98, u'subsidi-NN': 98, u'medium-NN': 97, u'applic-NN': 97, u'ownership-NN': 97, u'strategi-NN': 97, u'franc-NN': 96, u'summer-NN': 94, u'strength-NN': 94, u'commiss-NN': 93, u'afternoon-NN': 93, u'subcommitte-NN': 92, u'chanc-NN': 92, u'consider-NN': 92, u'factor-NN': 91, u'drought-NN': 91, u'author-NN': 91, u'engin-NN': 91, u'water-NN': 91, u'survey-NN': 90, u'maximum-NN': 88, u'spokeswoman-NN': 88, u'storag-NN': 87, u'worth-NN': 87, u'borrow-NN': 87, u'cours-NN': 85, u'chemic-NN': 85, u'condit-NN': 85, u'trader-NN': 84, u'inventori-NN': 84, u'commun-NN': 84, u'maker-NN': 84, u'build-NN': 84, u'spring-NN': 84, u'speech-NN': 83, u'employ-NN': 83, u'asset-NN': 81, u'smelter-NN': 81, u'recess-NN': 81, u'pretax-NN': 81, u'sorghum-NN': 81, u'ratio-NN': 80, u'indic-NN': 80, u'metal-NN': 80, u'commit-NN': 80, u'buyer-NN': 79, u'particip-NN': 79, u'deleg-NN': 79, u'sheet-NN': 78, u'defens-NN': 78, u'tariff-NN': 77, u'parti-NN': 77, u'expenditur-NN': 77, u'refin-NN': 77, u'confid-NN': 77, u'affili-NN': 77, u'depreci-NN': 77, u'pension-NN': 76, u'premium-NN': 76, u'direct-NN': 76, u'corpor-NN': 76, u'begin-NN': 76, u'brokerag-NN': 75, u'combin-NN': 75, u'protect-NN': 74, u'floor-NN': 74, u'opposit-NN': 74, u'relief-NN': 74, u'threat-NN': 73, u'anyth-NN': 73, u'mortgag-NN': 73, u'materi-NN': 72, u'stage-NN': 72, u'airlin-NN': 72, u'scale-NN': 72, u'staff-NN': 71, u'matter-NN': 71, u'auction-NN': 70, u'enhanc-NN': 70, u'writedown-NN': 69, u'shortfal-NN': 69, u'refer-NN': 69, u'light-NN': 68, u'freight-NN': 68, u'connect-NN': 68, u'march-NN': 68, u'termin-NN': 67, u'platform-NN': 67, u'governor-NN': 67, u'portion-NN': 66, u'visit-NN': 66, u'cannot-NN': 66, u'statu-NN': 65, u'reaction-NN': 65, u'contribut-NN': 65, u'aluminium-NN': 65, u'certif-NN': 65, u'emerg-NN': 64, u'sentiment-NN': 64, u'banker-NN': 64, u'partner-NN': 64, u'scheme-NN': 63, u'workforc-NN': 63, u'initi-NN': 63, u'sulphur-NN': 63, u'releas-NN': 62, u'structur-NN': 62, u'store-NN': 62, u'movement-NN': 62, u'compromis-NN': 62, u'leader-NN': 61, u'diseas-NN': 61, u'circul-NN': 61, u'potenti-NN': 60, u'extent-NN': 60, u'carryforward-NN': 60, u'debat-NN': 60, u'cabinet-NN': 60, u'uncertainti-NN': 60, u'communiqu-NN': 59, u'bankruptci-NN': 59, u'minor-NN': 59, u'rebat-NN': 59, u'extens-NN': 59, u'opinion-NN': 58, u'moment-NN': 58, u'opportun-NN': 58, u'public-NN': 58, u'network-NN': 57, u'transfer-NN': 57, u'labour-NN': 57, u'recommend-NN': 57, u'success-NN': 57, u'aircraft-NN': 57, u'backlog-NN': 56, u'introduct-NN': 56, u'reorgan-NN': 56, u'merchandis-NN': 56, u'exampl-NN': 56, u'expens-NN': 55, u'broker-NN': 55, u'benefit-NN': 55, u'elect-NN': 55, u'specialti-NN': 55, u'prefer-NN': 54, u'softwar-NN': 54, u'machineri-NN': 54, u'vessel-NN': 54, u'attent-NN': 54, u'payout-NN': 54, u'weight-NN': 53, u'arrang-NN': 53, u'textil-NN': 53, u'summit-NN': 53, u'campaign-NN': 53, u'ringgit-NN': 53, u'someth-NN': 53, u'chief-NN': 53, u'approach-NN': 52, u'organ-NN': 52, u'appreci-NN': 52, u'rapese-NN': 52, u'bushel-NN': 52, u'coast-NN': 52, u'advantag-NN': 52, u'secretari-NN': 52, u'portfolio-NN': 52, u'equival-NN': 52, u'repurchas-NN': 52, u'altern-NN': 51, u'solut-NN': 51, u'behalf-NN': 51, u'provinc-NN': 51, u'branch-NN': 50, u'video-NN': 50, u'tanker-NN': 50, u'dairi-NN': 50, u'collaps-NN': 50, u'dispos-NN': 49, u'recapit-NN': 49, u'block-NN': 49, u'receipt-NN': 48, u'convers-NN': 48, u'guilder-NN': 48, u'earthquak-NN': 48, u'investig-NN': 48, u'miner-NN': 48, u'shift-NN': 47, u'amend-NN': 47, u'aluminum-NN': 47, u'programm-NN': 47, u'panel-NN': 47, u'octan-NN': 47, u'protection-NN': 47, u'consortium-NN': 47, u'complaint-NN': 46, u'suspens-NN': 46, u'imbal-NN': 46, u'debit-NN': 46, u'china-NN': 46, u'retir-NN': 46, u'signal-NN': 46, u'advanc-NN': 45, u'platinum-NN': 45, u'retail-NN': 45, u'deadlin-NN': 45, u'section-NN': 45, u'defici-NN': 45, u'discuss-NN': 45, u'flour-NN': 45, u'radio-NN': 45, u'formula-NN': 45, u'doesn-NN': 44, u'livestock-NN': 44, u'depend-NN': 44, u'owner-NN': 44, u'resolut-NN': 44, u'deputi-NN': 44, u'outcom-NN': 44, u'chain-NN': 44, u'poultri-NN': 44, u'tonnag-NN': 44, u'station-NN': 44, u'interbank-NN': 43, u'island-NN': 43, u'withdraw-NN': 43, u'revis-NN': 43, u'courier-NN': 43, u'missil-NN': 43, u'disast-NN': 43, u'tobacco-NN': 42, u'labor-NN': 42, u'closur-NN': 42, u'repay-NN': 42, u'decreas-NN': 42, u'battl-NN': 42, u'mainten-NN': 42, u'margin-NN': 42, u'origin-NN': 42, u'exercis-NN': 42, u'remaind-NN': 41, u'excess-NN': 41, u'treatment-NN': 41, u'litig-NN': 41, u'moistur-NN': 41, u'minimum-NN': 41, u'midnight-NN': 41, u'ralli-NN': 40, u'critic-NN': 40, u'notic-NN': 40, u'repres-NN': 40, u'traffic-NN': 40, u'boost-NN': 40, u'warrant-NN': 40, u'breakdown-NN': 40, u'slide-NN': 40, u'presenc-NN': 40, u'holder-NN': 40, u'assumpt-NN': 40, u'exposur-NN': 40, u'center-NN': 39, u'compens-NN': 39, u'downturn-NN': 39, u'guarante-NN': 39, u'parliament-NN': 39, u'merchant-NN': 39, u'fleet-NN': 39, u'surpris-NN': 39, u'prospect-NN': 39, u'inflow-NN': 38, u'purpos-NN': 38, u'thing-NN': 38, u'ground-NN': 38, u'safeti-NN': 38, u'deregul-NN': 38, u'stanc-NN': 38, u'allow-NN': 38, u'schedul-NN': 38, u'subject-NN': 38, u'lumber-NN': 37, u'negoti-NN': 37, u'window-NN': 37, u'environ-NN': 37, u'depth-NN': 37, u'chapter-NN': 37, u'gallon-NN': 36, u'protest-NN': 36, u'advertis-NN': 36, u'prioriti-NN': 36, u'supplier-NN': 36, u'flexibl-NN': 36, u'method-NN': 36, u'break-NN': 36, u'registr-NN': 36, u'determin-NN': 36, u'carrier-NN': 36, u'whole-NN': 36, u'spread-NN': 35, u'coconut-NN': 35, u'practic-NN': 35, u'ferri-NN': 35, u'delay-NN': 35, u'willing-NN': 35, u'burden-NN': 35, u'reuter-NN': 35, u'headquart-NN': 35, u'licens-NN': 35, u'respect-NN': 35, u'devalu-NN': 35, u'deterior-NN': 34, u'event-NN': 34, u'differ-NN': 34, u'petrol-NN': 34, u'tonight-NN': 34, u'complex-NN': 34, u'defenc-NN': 34, u'broadcast-NN': 34, u'format-NN': 34, u'decre-NN': 33, u'consult-NN': 33, u'varieti-NN': 33, u'optim-NN': 33, u'membership-NN': 33, u'disclosur-NN': 33, u'liabil-NN': 33, u'revalu-NN': 33, u'momentum-NN': 33, u'alloc-NN': 32, u'factori-NN': 32, u'proport-NN': 32, u'contain-NN': 32, u'river-NN': 32, u'feasibl-NN': 32, u'phase-NN': 32, u'implement-NN': 32, u'oilse-NN': 32, u'distributor-NN': 32, u'petit-NN': 32, u'sunflow-NN': 32, u'slaughter-NN': 32, u'kwacha-NN': 32, u'belief-NN': 32, u'danger-NN': 32, u'segment-NN': 32, u'favor-NN': 31, u'content-NN': 31, u'organis-NN': 31, u'answer-NN': 31, u'conflict-NN': 31, u'advisor-NN': 31, u'print-NN': 31, u'advis-NN': 31, u'object-NN': 31, u'agent-NN': 31, u'lawsuit-NN': 31, u'discoveri-NN': 31, u'heart-NN': 31, u'expir-NN': 30, u'hotel-NN': 30, u'soymeal-NN': 30, u'relationship-NN': 30, u'revers-NN': 30, u'creditor-NN': 30, u'matur-NN': 30, u'scope-NN': 30, u'arbitrag-NN': 30, u'stabilis-NN': 30, u'document-NN': 30, u'flood-NN': 30, u'brand-NN': 30, u'protein-NN': 30, u'diplomat-NN': 30, u'climat-NN': 30, u'consent-NN': 30, u'autumn-NN': 30, u'cereal-NN': 29, u'coordin-NN': 29, u'adopt-NN': 29, u'friction-NN': 29, u'incent-NN': 29, u'midday-NN': 29, u'petrochem-NN': 29, u'proxi-NN': 29, u'cross-NN': 29, u'truck-NN': 29, u'draft-NN': 29, u'bidder-NN': 29, u'design-NN': 29, u'injunct-NN': 28, u'elimin-NN': 28, u'underwrit-NN': 28, u'counti-NN': 28, u'restraint-NN': 28, u'accept-NN': 28, u'charter-NN': 28, u'contact-NN': 28, u'apparel-NN': 28, u'freedom-NN': 28, u'spirit-NN': 28, u'border-NN': 28, u'consolid-NN': 28, u'consensu-NN': 28, u'slowdown-NN': 28, u'doubt-NN': 28, u'decad-NN': 28, u'agenda-NN': 28, u'basket-NN': 28, u'tension-NN': 28, u'confirm-NN': 27, u'award-NN': 27, u'conserv-NN': 27, u'north-NN': 27, u'difficulti-NN': 27, u'propan-NN': 27, u'comparison-NN': 27, u'privatis-NN': 27, u'accrual-NN': 27, u'natur-NN': 27, u'attitud-NN': 27, u'slump-NN': 27, u'trail-NN': 27, u'degre-NN': 27, u'insid-NN': 27, u'diamond-NN': 27, u'publish-NN': 27, u'appeal-NN': 27, u'bureau-NN': 27, u'pattern-NN': 27, u'complianc-NN': 26, u'float-NN': 26, u'counter-NN': 26, u'contrast-NN': 26, u'aerospac-NN': 26, u'stockpil-NN': 26, u'pictur-NN': 26, u'understand-NN': 26, u'volatil-NN': 26, u'stand-NN': 26, u'crown-NN': 26, u'telecommun-NN': 26, u'desir-NN': 26, u'resist-NN': 26, u'freez-NN': 26, u'placement-NN': 26, u'upturn-NN': 25, u'distanc-NN': 25, u'favour-NN': 25, u'histori-NN': 25, u'centr-NN': 25, u'entiti-NN': 25, u'passeng-NN': 25, u'motor-NN': 25, u'space-NN': 25, u'brief-NN': 25, u'coupon-NN': 25, u'poison-NN': 25, u'relianc-NN': 24, u'shell-NN': 24, u'orang-NN': 24, u'effici-NN': 24, u'person-NN': 24, u'entri-NN': 24, u'citru-NN': 24, u'choic-NN': 24, u'coverag-NN': 24, u'troubl-NN': 24, u'nickel-NN': 24, u'cartel-NN': 24, u'south-NN': 24, u'uranium-NN': 24, u'inspect-NN': 24, u'turnaround-NN': 24, u'liberalis-NN': 24, u'experi-NN': 23, u'default-NN': 23, u'debtor-NN': 23, u'oilfield-NN': 23, u'clear-NN': 23, u'avail-NN': 23, u'repli-NN': 23, u'sever-NN': 23, u'jungl-NN': 23, u'absenc-NN': 23, u'instrument-NN': 23, u'electr-NN': 23, u'bargain-NN': 23, u'aggreg-NN': 23, u'memori-NN': 23, u'waterway-NN': 23, u'expect-NN': 23, u'railway-NN': 23, u'mechan-NN': 23, u'popul-NN': 22, u'evalu-NN': 22, u'kingdom-NN': 22, u'transit-NN': 22, u'conclus-NN': 22, u'enforc-NN': 22, u'credibl-NN': 22, u'front-NN': 22, u'likelihood-NN': 22, u'testimoni-NN': 22, u'fiber-NN': 22, u'except-NN': 22, u'writeoff-NN': 22, u'competitor-NN': 22, u'exchequ-NN': 22, u'airport-NN': 22, u'emphasi-NN': 22, u'track-NN': 22, u'declar-NN': 22, u'spotlight-NN': 22, u'employe-NN': 22, u'articl-NN': 22, u'escort-NN': 22, u'realiti-NN': 21, u'assur-NN': 21, u'haven-NN': 21, u'accid-NN': 21, u'senat-NN': 21, u'microchip-NN': 21, u'issuanc-NN': 21, u'explos-NN': 21, u'inject-NN': 21, u'train-NN': 21, u'coupl-NN': 21, u'categori-NN': 21, u'vehicl-NN': 21, u'disposit-NN': 21, u'remov-NN': 21, u'sight-NN': 21, u'timber-NN': 21, u'permiss-NN': 21, u'fruit-NN': 21, u'shelf-NN': 21, u'centuri-NN': 20, u'debentur-NN': 20, u'railroad-NN': 20, u'procedur-NN': 20, u'count-NN': 20, u'feder-NN': 20, u'influenc-NN': 20, u'messag-NN': 20, u'bullion-NN': 20, u'tallow-NN': 20, u'temporao-NN': 20, u'arriv-NN': 20, u'outflow-NN': 20, u'taxat-NN': 20, u'shutdown-NN': 20, u'claim-NN': 20, u'languag-NN': 20, u'banner-NN': 20, u'shipbuild-NN': 20, u'bulletin-NN': 20, u'tighten-NN': 19, u'someon-NN': 19, u'physic-NN': 19, u'republ-NN': 19, u'everyth-NN': 19, u'deplet-NN': 19, u'concentr-NN': 19, u'forum-NN': 19, u'unrest-NN': 19, u'syndic-NN': 19, u'everyon-NN': 19, u'concess-NN': 19, u'commerc-NN': 19, u'april-NN': 19, u'specialist-NN': 19, u'gener-NN': 19, u'version-NN': 19, u'bauxit-NN': 19, u'drive-NN': 19, u'signific-NN': 19, u'anger-NN': 19, u'replac-NN': 19, u'manner-NN': 19, u'household-NN': 19, u'bottom-NN': 19, u'refus-NN': 19, u'ambassador-NN': 19, u'signup-NN': 19, u'caution-NN': 18, u'flotat-NN': 18, u'renew-NN': 18, u'integr-NN': 18, u'valuat-NN': 18, u'analysi-NN': 18, u'context-NN': 18, u'boycott-NN': 18, u'regul-NN': 18, u'salari-NN': 18, u'spell-NN': 18, u'inquiri-NN': 18, u'surfac-NN': 18, u'prepar-NN': 18, u'diesel-NN': 18, u'panic-NN': 18, u'quantiti-NN': 18, u'detail-NN': 18, u'underway-NN': 18, u'switch-NN': 18, u'mission-NN': 18, u'argument-NN': 18, u'redempt-NN': 18, u'seller-NN': 18, u'summari-NN': 17, u'foundat-NN': 17, u'gather-NN': 17, u'motion-NN': 17, u'princip-NN': 17, u'majeur-NN': 17, u'passag-NN': 17, u'challeng-NN': 17, u'fight-NN': 17, u'greenmail-NN': 17, u'counsel-NN': 17, u'continu-NN': 17, u'definit-NN': 17, u'instabl-NN': 17, u'buyback-NN': 17, u'violat-NN': 17, u'farmer-NN': 17, u'pilot-NN': 17, u'leadership-NN': 17, u'stockhold-NN': 17, u'magazin-NN': 17, u'escal-NN': 17, u'embassi-NN': 17, u'automobil-NN': 17, u'judgment-NN': 17, u'resal-NN': 17, u'protectionist-NN': 17, u'southwest-NN': 17, u'cloth-NN': 17, u'sugarcan-NN': 17, u'middl-NN': 17, u'countertrad-NN': 17, u'lobbi-NN': 17, u'condens-NN': 17, u'arbitrageur-NN': 17, u'plate-NN': 16, u'relat-NN': 16, u'olein-NN': 16, u'newslett-NN': 16, u'peseta-NN': 16, u'establish-NN': 16, u'renegoti-NN': 16, u'crash-NN': 16, u'outbreak-NN': 16, u'bread-NN': 16, u'framework-NN': 16, u'present-NN': 16, u'ethylen-NN': 16, u'warehous-NN': 16, u'pledg-NN': 16, u'refund-NN': 16, u'winterkil-NN': 16, u'conglomer-NN': 16, u'recognit-NN': 16, u'mouth-NN': 16, u'assess-NN': 16, u'forest-NN': 16, u'stream-NN': 16, u'machin-NN': 16, u'rumor-NN': 16, u'northeast-NN': 16, u'dilig-NN': 16, u'compon-NN': 16, u'navig-NN': 16, u'plywood-NN': 16, u'imposit-NN': 16, u'represent-NN': 15, u'conjunct-NN': 15, u'durum-NN': 15, u'percept-NN': 15, u'locat-NN': 15, u'footwear-NN': 15, u'cover-NN': 15, u'length-NN': 15, u'enrol-NN': 15, u'licenc-NN': 15, u'hectar-NN': 15, u'anticip-NN': 15, u'ethanol-NN': 15, u'receiv-NN': 15, u'crush-NN': 15, u'exist-NN': 15, u'correct-NN': 15, u'upland-NN': 15, u'destin-NN': 15, u'oblig-NN': 15, u'element-NN': 15, u'reluct-NN': 15, u'undersecretari-NN': 15, u'founder-NN': 15, u'suitor-NN': 15, u'venezuela-NN': 15, u'onshor-NN': 15, u'irrig-NN': 15, u'franchis-NN': 15, u'reject-NN': 15, u'butter-NN': 15, u'speed-NN': 15, u'stimul-NN': 15, u'departur-NN': 15, u'monopoli-NN': 15, u'disrupt-NN': 15, u'disappoint-NN': 15, u'storm-NN': 15, u'glass-NN': 15, u'arbitr-NN': 15, u'turbul-NN': 14, u'promot-NN': 14, u'explan-NN': 14, u'alloy-NN': 14, u'inter-NN': 14, u'prospectu-NN': 14, u'candid-NN': 14, u'swing-NN': 14, u'waiver-NN': 14, u'casualti-NN': 14, u'rationalis-NN': 14, u'stoppag-NN': 14, u'alumina-NN': 14, u'audit-NN': 14, u'pickup-NN': 14, u'laboratori-NN': 14, u'independ-NN': 14, u'marketplac-NN': 14, u'resort-NN': 14, u'victori-NN': 14, u'involv-NN': 14, u'regim-NN': 14, u'truste-NN': 14, u'resourc-NN': 14, u'stimulu-NN': 14, u'grant-NN': 14, u'chicken-NN': 14, u'spinoff-NN': 14, u'adher-NN': 14, u'stori-NN': 14, u'district-NN': 14, u'confront-NN': 14, u'rupiah-NN': 14, u'scandal-NN': 14, u'bridg-NN': 14, u'alcohol-NN': 14, u'fertil-NN': 14, u'probe-NN': 14, u'copra-NN': 14, u'procur-NN': 14, u'launch-NN': 14, u'carbon-NN': 14, u'diversif-NN': 14, u'fertilis-NN': 13, u'entertain-NN': 13, u'conting-NN': 13, u'pasta-NN': 13, u'salmonella-NN': 13, u'advic-NN': 13, u'enterpris-NN': 13, u'farmland-NN': 13, u'fraud-NN': 13, u'breakup-NN': 13, u'divestitur-NN': 13, u'netback-NN': 13, u'assembl-NN': 13, u'suffici-NN': 13, u'suggest-NN': 13, u'plung-NN': 13, u'instal-NN': 13, u'clearanc-NN': 13, u'hospit-NN': 13, u'devic-NN': 13, u'fluctuat-NN': 13, u'offtak-NN': 13, u'drain-NN': 13, u'horizon-NN': 13, u'sunflowerse-NN': 13, u'supermarket-NN': 13, u'prorat-NN': 13, u'businessman-NN': 13, u'regist-NN': 13, u'hardship-NN': 13, u'nobodi-NN': 13, u'sweeten-NN': 13, u'royalti-NN': 13, u'tight-NN': 13, u'anyon-NN': 13, u'frustrat-NN': 13, u'travel-NN': 13, u'restaur-NN': 13, u'worker-NN': 13, u'surveil-NN': 13, u'potato-NN': 13, u'furnitur-NN': 13, u'concept-NN': 13, u'realign-NN': 13, u'patent-NN': 13, u'moratorium-NN': 12, u'rebound-NN': 12, u'casino-NN': 12, u'naphtha-NN': 12, u'unwilling-NN': 12, u'starch-NN': 12, u'weaken-NN': 12, u'inabl-NN': 12, u'telex-NN': 12, u'chart-NN': 12, u'topic-NN': 12, u'windfal-NN': 12, u'beverag-NN': 12, u'extinguish-NN': 12, u'trial-NN': 12, u'canal-NN': 12, u'goodwil-NN': 12, u'anniversari-NN': 12, u'argosystem-NN': 12, u'ocean-NN': 12, u'aviat-NN': 12, u'cathod-NN': 12, u'auster-NN': 12, u'memorandum-NN': 12, u'controversi-NN': 12, u'supertank-NN': 12, u'inclus-NN': 12, u'style-NN': 12, u'transmiss-NN': 12, u'butan-NN': 12, u'creation-NN': 12, u'reorganis-NN': 12, u'slack-NN': 12, u'southeast-NN': 12, u'incid-NN': 12, u'dilemma-NN': 12, u'ration-NN': 12, u'financi-NN': 12, u'compound-NN': 12, u'scrap-NN': 12, u'repeal-NN': 12, u'holiday-NN': 12, u'territori-NN': 12, u'check-NN': 12, u'knowledg-NN': 12, u'scenario-NN': 12, u'resign-NN': 12, u'fortun-NN': 12, u'injuri-NN': 11, u'claus-NN': 11, u'faith-NN': 11, u'cutback-NN': 11, u'liber-NN': 11, u'kernel-NN': 11, u'commonwealth-NN': 11, u'smelt-NN': 11, u'disciplin-NN': 11, u'fraction-NN': 11, u'payrol-NN': 11, u'nakason-NN': 11, u'contin-NN': 11, u'rainfal-NN': 11, u'acceler-NN': 11, u'laser-NN': 11, u'tendenc-NN': 11, u'hardwar-NN': 11, u'rescu-NN': 11, u'strengthen-NN': 11, u'judgement-NN': 11, u'phone-NN': 11, u'resumpt-NN': 11, u'restat-NN': 11, u'stretch-NN': 11, u'yearend-NN': 11, u'reflect-NN': 11, u'treasur-NN': 11, u'concert-NN': 11, u'remark-NN': 11, u'lanka-NN': 11, u'stress-NN': 11, u'envoy-NN': 11, u'cyacq-NN': 11, u'promis-NN': 11, u'blood-NN': 11, u'dinner-NN': 11, u'lockout-NN': 11, u'dilut-NN': 11, u'street-NN': 11, u'resurg-NN': 11, u'standard-NN': 11, u'tonner-NN': 11, u'catalog-NN': 11, u'batteri-NN': 11, u'ecuador-NN': 11, u'sovereignti-NN': 11, u'dozen-NN': 11, u'incorpor-NN': 11, u'lender-NN': 11}) target nouns\n"
     ]
    }
   ],
   "source": [
    "#Than ignore all the nouns that are shorter than 3 characters\n",
    "selected_stemmed_noun_frequencies = Counter()\n",
    "for k,v in stemmed_noun_frequencies.most_common():\n",
    "    if len(k) < 8:  \n",
    "        continue\n",
    "    elif v == 10:\n",
    "        break\n",
    "    else:\n",
    "        selected_stemmed_noun_frequencies[k] = v\n",
    "        \n",
    "print(selected_stemmed_noun_frequencies), \"target nouns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'profit-NN', u'loss-NN'), 1831), ((u'profit-NN', u'profit-NN'), 782), ((u'profit-NN', u'shr-NN'), 654), ((u'trade-NN', u'S-NN'), 325), ((u'tender-NN', u'offer-NN'), 306), ((u'offer-NN', u'tender-NN'), 306), ((u'market-NN', u'money-NN'), 302), ((u'money-NN', u'market-NN'), 302), ((u'buffer-NN', u'stock-NN'), 284), ((u'stock-NN', u'buffer-NN'), 284), ((u'split-NN', u'stock-NN'), 251), ((u'stock-NN', u'split-NN'), 251), ((u'deficit-NN', u'trade-NN'), 249), ((u'trade-NN', u'deficit-NN'), 249), ((u'exchang-NN', u'rate-NN'), 233), ((u'profit-NN', u'year-NN'), 225), ((u'trade-NN', u'surplu-NN'), 221), ((u'surplu-NN', u'trade-NN'), 221), ((u'money-NN', u'suppli-NN'), 213), ((u'suppli-NN', u'money-NN'), 213)]\n"
     ]
    }
   ],
   "source": [
    "filtered_stemmed_cooccs_surface = Counter()\n",
    "#Now find the most interesting coocurencies by combining the two filters introduced above\n",
    "\n",
    "for k,v in cooccs_surface.iteritems():\n",
    "    if selected_stemmed_noun_frequencies.has_key(k[0]) and filtered_stemmed_frequencies.has_key(k[1]):\n",
    "        filtered_stemmed_cooccs_surface[k] = v\n",
    "        \n",
    "print filtered_stemmed_cooccs_surface.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lemmatized version\n",
    "#First lemmatize the reuters corpora and then follow the same steps as with the stemmed version\n",
    "\n",
    "un2wn_mapping = {\"VERB\" : \"v\", \"NOUN\" : \"n\", \"ADJ\" : \"a\", \"ADV\" : \"r\"}\n",
    "\n",
    "reuters_lemmatized = []\n",
    "\n",
    "for i in range(len(reuter_PoSTagged)):\n",
    "    lemmatized_list = []\n",
    "    for w, p in reuter_PoSTagged[i]:\n",
    "        if p in [\".\", \"X\"]:\n",
    "            continue\n",
    "        elif p in un2wn_mapping.keys():\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w, pos = un2wn_mapping[p])\n",
    "        else:\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w)\n",
    "        i += 1\n",
    "        lemmatized_list.append(\"-\".join([lemma, p]))\n",
    "        \n",
    "    reuters_lemmatized.append(lemmatized_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find permutations\n",
    "cooccs_textual = Counter()\n",
    "\n",
    "for sentence in reuters_lemmatized:\n",
    "    cooccs_textual.update([pair for pair in set(permutations(sentence, 2)) if pair[0].split(\"-\")[-1] == \"NN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find cocurencies in span size 5 words on each side of the word\n",
    "spansize = 5\n",
    "reuters_cooccs_surface = Counter()\n",
    "\n",
    "for sentence in reuters_lemmatized:\n",
    "    for i,w in enumerate(sentence):\n",
    "        if w.split(\"-\")[-1] == \"NN\":\n",
    "            span_range = range(max(i- spansize, 0), i)  # 5 words on the left \n",
    "            span_range.extend(range(i+1, min(i + spansize + 1, len(sentence))))  # and 5 words on the right\n",
    "            for cw in [sentence[idx] for idx in span_range]:\n",
    "                reuters_cooccs_surface[(w, cw)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a count of the frequencies of the lemmas and the nouns\n",
    "reuters_lemmas_frequencies = Counter(chain(*reuters_lemmatized))\n",
    "reuters_noun_frequencies = Counter([w for w in chain(*reuters_lemmatized) if w.split(\"-\")[-1] == \"NN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2845 context lemmas\n",
      "[(u'lt-NN', 8592), (u'year-NN', 6310), (u'S-NN', 6045), (u'loss-NN', 4528), (u'company-NN', 4399), (u'share-NN', 2896), (u'profit-NN', 2812), (u'oil-NN', 2518), (u'market-NN', 2484), (u'stock-NN', 2329), (u'trade-NN', 2106), (u'quarter-NN', 1766), (u'price-NN', 1658), (u'rate-NN', 1624), (u'government-NN', 1567), (u'week-NN', 1538), (u'bank-NN', 1472), (u'dollar-NN', 1462), (u'agreement-NN', 1336), (u'today-NN', 1320), (u'tax-NN', 1310), (u'offer-NN', 1307), (u'production-NN', 1283), (u'month-NN', 1266), (u'interest-NN', 1253), (u'group-NN', 1140), (u'dlr-NN', 1119), (u'stg-NN', 1102), (u'growth-NN', 1019), (u',\"-NN', 1018), (u'exchange-NN', 1004), (u'.\"-NN', 989), (u'meeting-NN', 978), (u'spokesman-NN', 952), (u'end-NN', 934), (u'Avg-NN', 914), (u'cash-NN', 911), (u'sale-NN', 910), (u'board-NN', 882), (u'industry-NN', 833), (u'world-NN', 794), (u'investment-NN', 793), (u'business-NN', 792), (u'export-NN', 789), (u'time-NN', 786), (u'statement-NN', 784), (u'wheat-NN', 766), (u'debt-NN', 764), (u'rise-NN', 760), (u'deficit-NN', 747), (u'NOTE-VB', 733), (u'money-NN', 733), (u'shr-NN', 728), (u'increase-NN', 728), (u'period-NN', 726), (u'dividend-NN', 698), (u'record-NN', 697), (u'buy-VB', 692), (u'report-NN', 685), (u'gain-NN', 682)]\n"
     ]
    }
   ],
   "source": [
    "#Filtering\n",
    "filtered_reuters_lemmas_frequencies = Counter()\n",
    "\n",
    "#First exclude the closed class words, and the words that are in the specified list\n",
    "#The specific words in the list are words that do not give any information about the text\n",
    "#Either because they are verbs like to be or to have, the words: mln,  pct, Shr, QTR were very frequent though do not mean anything\n",
    "filtered_cooccs_surface = Counter()\n",
    "\n",
    "for k,v in reuters_lemmas_frequencies.iteritems():\n",
    "    if v >= 10 and \\\n",
    "    k.split(\"-\")[-1] in [\"NN\", \"VB\", \"VBG\", \"ADJ\", \"ADV\"]\\\n",
    "    and k.split(\"-\")[0] not in [\"be\", \"have\", \"do\", \"would\", \"will\", \"could\", \"mln\", \"pct\", \"Shr\", \"QTR\"]:\n",
    "        filtered_reuters_lemmas_frequencies[k] = v\n",
    "\n",
    "print len(filtered_reuters_lemmas_frequencies), \"context lemmas\"\n",
    "print filtered_reuters_lemmas_frequencies.most_common(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'company-NN', 4399), (u'share-NN', 2896), (u'profit-NN', 2812), (u'market-NN', 2484), (u'stock-NN', 2329), (u'trade-NN', 2106), (u'quarter-NN', 1766), (u'price-NN', 1658), (u'government-NN', 1567), (u'dollar-NN', 1462), (u'agreement-NN', 1336), (u'today-NN', 1320), (u'offer-NN', 1307), (u'production-NN', 1283), (u'month-NN', 1266), (u'interest-NN', 1253), (u'group-NN', 1140), (u'growth-NN', 1019), (u'exchange-NN', 1004), (u'meeting-NN', 978)] target nouns\n"
     ]
    }
   ],
   "source": [
    "#Than ignore all the lemma's that are shorter than 3 characters\n",
    "selected_reuters_noun_frequencies = Counter()\n",
    "for k,v in reuters_noun_frequencies.most_common():\n",
    "    if len(k) < 8:  \n",
    "        continue\n",
    "    elif v == 10:\n",
    "        break\n",
    "    else:\n",
    "        selected_reuters_noun_frequencies[k] = v\n",
    "        \n",
    "print(selected_reuters_noun_frequencies.most_common(20)), \"target nouns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'profit-NN', u'loss-NN'), 1825), ((u'profit-NN', u'profit-NN'), 782), ((u'tender-NN', u'offer-NN'), 300), ((u'offer-NN', u'tender-NN'), 300), ((u'buffer-NN', u'stock-NN'), 273), ((u'stock-NN', u'buffer-NN'), 273), ((u'market-NN', u'money-NN'), 262), ((u'money-NN', u'market-NN'), 262), ((u'split-NN', u'stock-NN'), 251), ((u'stock-NN', u'split-NN'), 251), ((u'exchange-NN', u'rate-NN'), 214), ((u'trade-NN', u'S-NN'), 210), ((u'deficit-NN', u'trade-NN'), 207), ((u'trade-NN', u'deficit-NN'), 207), ((u'surplus-NN', u'trade-NN'), 187), ((u'trade-NN', u'surplus-NN'), 187), ((u'profit-NN', u'shr-NN'), 180), ((u'supply-NN', u'money-NN'), 173), ((u'money-NN', u'supply-NN'), 173), ((u'interest-NN', u'rate-NN'), 164)]\n"
     ]
    }
   ],
   "source": [
    "filtered_cooccs_surface = Counter()\n",
    "#Now find the most interesting coocurencies by combining the two filters introduced above\n",
    "\n",
    "for k,v in reuters_cooccs_surface.iteritems():\n",
    "    if selected_reuters_noun_frequencies.has_key(k[0]) and filtered_reuters_lemmas_frequencies.has_key(k[1]):\n",
    "        filtered_cooccs_surface[k] = v\n",
    "        \n",
    "print filtered_cooccs_surface.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Answers to the question:\n",
    "#vectors ?\n",
    "#how do you want to describe them\n",
    "#what filtering strategy are you going to choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. (15 points)\n",
    "\n",
    "Weight the counts in the two spaces created in the previous exercise by using the following association measure:\n",
    "\n",
    "\n",
    "- one measure of your choice among those available in the [nltk.BigramAssocMeasures](http://www.nltk.org/howto/metrics.html#association-measures) module (**not the pmi**) \n",
    "\n",
    "\n",
    "- the **positive local mutual information**\n",
    "\n",
    "\n",
    "- the smoothed ppmi proposed by [Levy et al. (2015)](http://www.aclweb.org/anthology/Q15-1016) discussed in class. Please recall that these scholars proposed to smooth the ppmi by raising the context counts to the power of $\\alpha$ (where $\\alpha= 0.75$ is reported to work well). That is, if $V_c$ is the vocabulary of all the contexts in a given space, they proposed the following association measure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PPMI_\\alpha (w,c) = max \\left(0, \\ log_2 \\left(\\frac{p(w,c)}{p(w) * p_\\alpha(c)}\\right)  \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$where: \\ \\ p_\\alpha(c) = \\frac{f(c)^\\alpha}{\\sum_{c' \\in V_c} f(c')^\\alpha}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "from math import log\n",
    "\n",
    "# All functions receive as arguments:\n",
    "#\n",
    "#   o_11 >> the cooccurrences of 2 items >> (w1, w2)\n",
    "#   r_1 >> w1 marginals >> (w1, *)\n",
    "#   c_1 >> w2 marginals >> (*, w2)\n",
    "#   n >> total number of possible coccorrencies >> (*, *)\n",
    "\n",
    "# These arguments can be arranged in a contingency table of the form:\n",
    "#\n",
    "#          w2    ~w2\n",
    "#        ------ ------\n",
    "#    w1 | o_11 | o_12 | = r_1\n",
    "#        ------ ------\n",
    "#   ~w1 | o_21 | o_22 | = r_2\n",
    "#        ------ ------\n",
    "#        = c_1  = c_2   = n\n",
    "\n",
    "#Positive Pointwise Mutual Information (Church & Hanks, 1990)\n",
    "def ppmi(o_11, r_1, c_1, n):\n",
    "    observed = o_11\n",
    "    expected = (r_1*c_1)/n \n",
    "    res = log(observed/expected,2)\n",
    "    return max(0, res)\n",
    "\n",
    "\n",
    "#Positive Local Mutual Information\n",
    "def plmi(o_11, r_1, c_1, n):\n",
    "    res = o_11 * ppmi(o_11, r_1, c_1, n)\n",
    "    return res\n",
    "\n",
    "#Chi Squared Bigram Associated Measure\n",
    "def chi_measure(o_11, r_1, c_1, n):\n",
    "    bam = nltk.collocations.BigramAssocMeasures\n",
    "    res = bam.chi_sq(o_11, (r_1, c_1), n)\n",
    "    return res    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'lanka-NN', u'sri-NN'), 17.34457114034622), ((u'rainfal-NN', u'rainfal-NN'), 17.26365114496265), ((u'poison-NN', u'pill-NN'), 16.53865819246252), ((u'trail-NN', u'kimberley-NN'), 16.427626880073777), ((u'olein-NN', u'rbd-NN'), 16.416979635874267), ((u'memorandum-NN', u'understand-VBG'), 16.334517475682297), ((u'inspect-NN', u'bird-NN'), 16.012589380794935), ((u'stretch-NN', u'film-NN'), 15.875085857044999), ((u'count-NN', u'rig-NN'), 15.76105061379897), ((u'uranium-NN', u'oxid-NN'), 15.723082763599947), ((u'windfal-NN', u'repeal-NN'), 15.59755188151609), ((u'repeal-NN', u'windfal-NN'), 15.59755188151609), ((u'suffici-NN', u'self-NN'), 15.567804538122036), ((u'ethanol-NN', u'exempt-NN'), 15.53865819246252), ((u'definit-NN', u'revis-VBG'), 15.510089040265749), ((u'footwear-NN', u'cloth-NN'), 15.510089040265749), ((u'cloth-NN', u'footwear-NN'), 15.510089040265749), ((u'casino-NN', u'hotel-NN'), 15.498016207965176), ((u'hotel-NN', u'casino-NN'), 15.498016207965176), ((u'deplet-NN', u'allow-NN'), 15.494014277407679)]\n"
     ]
    }
   ],
   "source": [
    "# PPMI for the stemmed version\n",
    "\n",
    "ppmis_surface_1 = Counter()\n",
    "\n",
    "N = sum(cooccs_surface.itervalues())  \n",
    "\n",
    "for k,v in filtered_stemmed_cooccs_surface.iteritems():\n",
    "    ppmis_surface_1[k] = ppmi(v, stemmed_frequencies[k[0]], stemmed_frequencies[k[1]], N)\n",
    "    \n",
    "print ppmis_surface_1.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'profit-NN', u'loss-NN'), 15237.007863025967), ((u'profit-NN', u'profit-NN'), 6090.896354974207), ((u'profit-NN', u'shr-NN'), 5153.786865616063), ((u'tender-NN', u'offer-NN'), 2934.3865876671566), ((u'offer-NN', u'tender-NN'), 2934.3865876671566), ((u'buffer-NN', u'stock-NN'), 2828.5603321408344), ((u'stock-NN', u'buffer-NN'), 2828.5603321408344), ((u'market-NN', u'money-NN'), 2495.6159390868643), ((u'money-NN', u'market-NN'), 2495.6159390868643), ((u'split-NN', u'stock-NN'), 2343.1033925710494), ((u'stock-NN', u'split-NN'), 2343.1033925710494), ((u'money-NN', u'suppli-NN'), 2200.702936518645), ((u'suppli-NN', u'money-NN'), 2200.702936518645), ((u'deficit-NN', u'trade-NN'), 1975.627607361494), ((u'trade-NN', u'deficit-NN'), 1975.627607361494), ((u'exchang-NN', u'rate-NN'), 1918.7403238965671), ((u'surplu-NN', u'trade-NN'), 1772.295691190822), ((u'trade-NN', u'surplu-NN'), 1772.295691190822), ((u'trade-NN', u'S-NN'), 1756.4537891188065), ((u'confer-NN', u'news-NN'), 1600.9454585822227)]\n"
     ]
    }
   ],
   "source": [
    "# Positive local mutual information for the stemmed version\n",
    "\n",
    "plmis_surface_1 = Counter()\n",
    "\n",
    "N = sum(cooccs_surface.itervalues())  \n",
    "\n",
    "for k,v in filtered_stemmed_cooccs_surface.iteritems():\n",
    "    plmis_surface_1[k] = plmi(v, stemmed_frequencies[k[0]], stemmed_frequencies[k[1]], N)\n",
    "    \n",
    "print plmis_surface_1.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'lanka-NN', u'sri-NN'), 1664314.2657198946), ((u'poison-NN', u'pill-NN'), 1523177.5999612769), ((u'rainfal-NN', u'rainfal-NN'), 1258824.8594877454), ((u'trail-NN', u'kimberley-NN'), 881465.5555291001), ((u'olein-NN', u'rbd-NN'), 874982.5734925908), ((u'count-NN', u'rig-NN'), 777446.5065913154), ((u'uranium-NN', u'oxid-NN'), 649072.3635763171), ((u'deplet-NN', u'allow-NN'), 646074.5982589573), ((u'allow-NN', u'deplet-NN'), 646074.5982589573), ((u'suffici-NN', u'self-NN'), 631412.1223968093), ((u'decre-NN', u'consent-NN'), 615411.9110036844), ((u'consent-NN', u'decre-NN'), 615411.9110036844), ((u'orang-NN', u'juic-NN'), 607375.4895011195), ((u'profit-NN', u'loss-NN'), 584028.1845461781), ((u'intent-NN', u'letter-NN'), 563615.0306576936), ((u'letter-NN', u'intent-NN'), 563615.0306576936), ((u'inspect-NN', u'bird-NN'), 528875.11108422), ((u'smelter-NN', u'trail-NN'), 479892.23026863876), ((u'trail-NN', u'smelter-NN'), 479892.23026863876), ((u'confer-NN', u'news-NN'), 475361.5415814042)]\n"
     ]
    }
   ],
   "source": [
    "# The Chi square method for the stemmed version\n",
    "\n",
    "chi_surface_1 = Counter()\n",
    "N = sum(cooccs_surface.itervalues())\n",
    "\n",
    "for k,v in filtered_stemmed_cooccs_surface.iteritems():\n",
    "    chi_surface_1[k] = chi_measure(v, stemmed_frequencies[k[0]], stemmed_frequencies[k[1]], N)  \n",
    "    \n",
    "print chi_surface_1.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'poison-NN', u'pill-NN'), 17.09505154098691), ((u'Trail-NN', u'Kimberley-NN'), 16.427626880073777), ((u'setting-NN', u'pod-NN'), 16.26365114496265), ((u'Supply-NN', u'Ttl-NN'), 16.21673009757516), ((u'inspection-NN', u'bird-NN'), 16.138120262878793), ((u'olein-NN', u'RBD-NN'), 16.045010858487313), ((u'BALANCE-NN', u'PORT-NN'), 15.986117169433742), ((u'stretch-NN', u'film-NN'), 15.957548017236972), ((u'count-NN', u'rig-NN'), 15.925126539544594), ((u'SYSTEM-NN', u'DAY-NN'), 15.860586287349884), ((u'navigation-NN', u'freedom-NN'), 15.81619216799143), ((u'freedom-NN', u'navigation-NN'), 15.81619216799143), ((u'Enhancement-NN', u'Program-NN'), 15.81594237229504), ((u'Program-NN', u'Enhancement-NN'), 15.81594237229504), ((u'oxide-NN', u'uranium-NN'), 15.784483308264091), ((u'uranium-NN', u'oxide-NN'), 15.784483308264091), ((u'sufficiency-NN', u'self-NN'), 15.756249627535148), ((u'orange-NN', u'juice-NN'), 15.713744899020611), ((u'juice-NN', u'orange-NN'), 15.713744899020611), ((u'footwear-NN', u'clothing-NN'), 15.690661285907572)]\n"
     ]
    }
   ],
   "source": [
    "# PPMI for the lemmatized version\n",
    "\n",
    "ppmis_surface_2 = Counter()\n",
    "\n",
    "N = sum(reuters_cooccs_surface.itervalues())  \n",
    "\n",
    "for k,v in filtered_cooccs_surface.iteritems():\n",
    "    ppmis_surface_2[k] = ppmi(v, reuters_lemmas_frequencies[k[0]], reuters_lemmas_frequencies[k[1]], N)\n",
    "    \n",
    "print ppmis_surface_2.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'profit-NN', u'loss-NN'), 15355.834716402314), ((u'profit-NN', u'profit-NN'), 6161.20394353443), ((u'tender-NN', u'offer-NN'), 2963.0685880599494), ((u'offer-NN', u'tender-NN'), 2963.0685880599494), ((u'buffer-NN', u'stock-NN'), 2732.0823232227144), ((u'stock-NN', u'buffer-NN'), 2732.0823232227144), ((u'split-NN', u'stock-NN'), 2360.9864675267754), ((u'stock-NN', u'split-NN'), 2360.9864675267754), ((u'market-NN', u'money-NN'), 2205.9924568041456), ((u'money-NN', u'market-NN'), 2205.9924568041456), ((u'supply-NN', u'money-NN'), 1810.7006982553157), ((u'money-NN', u'supply-NN'), 1810.7006982553157), ((u'exchange-NN', u'rate-NN'), 1773.439806065523), ((u'deficit-NN', u'trade-NN'), 1716.1846674857748), ((u'trade-NN', u'deficit-NN'), 1716.1846674857748), ((u'conference-NN', u'news-NN'), 1607.5362210803264), ((u'surplus-NN', u'trade-NN'), 1567.2058770094748), ((u'trade-NN', u'surplus-NN'), 1567.2058770094748), ((u'president-NN', u'vice-NN'), 1478.47251778333), ((u'intent-NN', u'letter-NN'), 1409.8604775701929)]\n"
     ]
    }
   ],
   "source": [
    "# Positive local mutual information for the lemmatized version\n",
    "\n",
    "plmis_surface_2 = Counter()\n",
    "\n",
    "N = sum(reuters_cooccs_surface.itervalues())  \n",
    "\n",
    "for k,v in filtered_cooccs_surface.iteritems():\n",
    "    plmis_surface_2[k] = plmi(v, reuters_lemmas_frequencies[k[0]], reuters_lemmas_frequencies[k[1]], N)\n",
    "    \n",
    "print plmis_surface_2.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'poison-NN', u'pill-NN'), 2099975.294106525), ((u'Enhancement-NN', u'Program-NN'), 2019016.7381426909), ((u'Program-NN', u'Enhancement-NN'), 2019016.7381426909), ((u'Trail-NN', u'Kimberley-NN'), 881465.5555291001), ((u'intent-NN', u'letter-NN'), 853215.9028794424), ((u'letter-NN', u'intent-NN'), 853215.9028794424), ((u'SYSTEM-NN', u'DAY-NN'), 832979.6999270576), ((u'MONEY-NN', u'SUPPLY-NN'), 770838.7903300002), ((u'SUPPLY-NN', u'MONEY-NN'), 770838.7903300002), ((u'orange-NN', u'juice-NN'), 752366.999923634), ((u'juice-NN', u'orange-NN'), 752366.999923634), ((u'count-NN', u'rig-NN'), 746648.3136693434), ((u'sufficiency-NN', u'self-NN'), 719517.9534388306), ((u'oxide-NN', u'uranium-NN'), 677293.6600195062), ((u'uranium-NN', u'oxide-NN'), 677293.6600195062), ((u'allowance-NN', u'depletion-NN'), 646074.5982589573), ((u'depletion-NN', u'allowance-NN'), 646074.5982589573), ((u'profit-NN', u'loss-NN'), 620821.4452809275), ((u'decree-NN', u'consent-NN'), 615411.9110036844), ((u'consent-NN', u'decree-NN'), 615411.9110036844)]\n"
     ]
    }
   ],
   "source": [
    "# The Chi square method for the lemmatized version\n",
    "\n",
    "chi_surface_2 = Counter()\n",
    "N = sum(reuters_cooccs_surface.itervalues())\n",
    "\n",
    "for k,v in filtered_cooccs_surface.iteritems():\n",
    "    chi_surface_2[k] = chi_measure(v, reuters_lemmas_frequencies[k[0]], reuters_lemmas_frequencies[k[1]], N)  \n",
    "    \n",
    "print chi_surface_2.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now make make vectors of the two spaces and apply the measures on them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for stemmed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemma 2 rows/column index mapppings\n",
    "sorted_vectors = sorted(selected_stemmed_noun_frequencies)\n",
    "vectors_indices = dict((v,i) for i,v in enumerate(sorted_vectors))\n",
    "contexts_indices = dict((v,i) for i,v in enumerate(sorted(filtered_stemmed_frequencies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the PPMI Weighted dense matrix\n",
    "ppmiMat = np.zeros((len(vectors_indices), len(contexts_indices)))\n",
    "\n",
    "# populate the matrix\n",
    "for pair, weight in ppmis_surface_1.iteritems():\n",
    "    ppmiMat[vectors_indices[pair[0]]][contexts_indices[pair[1]]] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.50920531  0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 7.10202993  0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print ppmiMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the PLMI Weighted dense matrix\n",
    "plmiMat = np.zeros((len(vectors_indices), len(contexts_indices)))\n",
    "\n",
    "# populate the matrix\n",
    "for pair, weight in plmis_surface_1.iteritems():\n",
    "    plmiMat[vectors_indices[pair[0]]][contexts_indices[pair[1]]] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the LL Weighted dense matrix\n",
    "chiMat = np.zeros((len(vectors_indices), len(contexts_indices)))\n",
    "\n",
    "# populate the matrix\n",
    "for pair, weight in chi_surface_1.iteritems():\n",
    "    chiMat[vectors_indices[pair[0]]][contexts_indices[pair[1]]] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now for the lemmatized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemma 2 rows/column index mapppings\n",
    "sorted_vectors = sorted(selected_reuters_noun_frequencies)\n",
    "vectors_indices = dict((v,i) for i,v in enumerate(sorted_vectors))\n",
    "contexts_indices = dict((v,i) for i,v in enumerate(sorted(filtered_reuters_lemmas_frequencies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the PPMI Weighted dense matrix\n",
    "ppmiMat_2 = np.zeros((len(vectors_indices), len(contexts_indices)))\n",
    "\n",
    "# populate the matrix\n",
    "for pair, weight in ppmis_surface_2.iteritems():\n",
    "    ppmiMat_2[vectors_indices[pair[0]]][contexts_indices[pair[1]]] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the PLMI Weighted dense matrix\n",
    "plmiMat_2 = np.zeros((len(vectors_indices), len(contexts_indices)))\n",
    "\n",
    "# populate the matrix\n",
    "for pair, weight in plmis_surface_2.iteritems():\n",
    "    plmiMat_2[vectors_indices[pair[0]]][contexts_indices[pair[1]]] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the LL Weighted dense matrix\n",
    "chiMat_2 = np.zeros((len(vectors_indices), len(contexts_indices)))\n",
    "\n",
    "# populate the matrix\n",
    "for pair, weight in chi_surface_2.iteritems():\n",
    "    chiMat_2[vectors_indices[pair[0]]][contexts_indices[pair[1]]] = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4. (15 points)\n",
    "\n",
    "Up to this point, you should have 6 very sparse distributional spaces. \n",
    "\n",
    "Use **Singular Value Decomposition** to reduce their dimensionality by:\n",
    "\n",
    "\n",
    "- retaining only the first 50 dimensions\n",
    "\n",
    "\n",
    "- retaining only the first 300 dimensions.\n",
    "\n",
    "\n",
    "- retaining the first 50 dimensions, with the exclusion of the first 5 (i.e. retaining the dimensions 6-50). For this setting, follow the methodology proposed by Jurafsky & Martin (2018) and ignore $\\Sigma$ (i.e. the matrix with the singular values) when building the truncated matrix for the target vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVD with first 50 dimensions\n",
    "dimensions_50 = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Aplly SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On stemmed version:\n",
    "ppmiMat_tr50 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_50, algorithm = \"arpack\").fit\n",
    "plmiMat_tr50 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_50, algorithm = \"arpack\").fit_transform(plmiMat)\n",
    "chiMat_tr50 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_50, algorithm = \"arpack\").fit_transform(chiMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On lemmatized version:\n",
    "ppmiMat_2_tr50 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_50, algorithm = \"arpack\").fit_transform(ppmiMat_2)\n",
    "plmiMat_2_tr50 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_50, algorithm = \"arpack\").fit_transform(plmiMat_2)\n",
    "chiMat_2_tr50 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_50, algorithm = \"arpack\").fit_transform(chiMat_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVD with first 300 dimensions\n",
    "dimensions_300 = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for stemmed version:\n",
    "ppmiMat_tr300 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_300, algorithm = \"arpack\").fit_transform(ppmiMat)\n",
    "plmiMat_tr300 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_300, algorithm = \"arpack\").fit_transform(plmiMat)\n",
    "chiMat_tr300 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_300, algorithm = \"arpack\").fit_transform(chiMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On the lemmatized version:\n",
    "ppmiMat_2_tr300 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_300, algorithm = \"arpack\").fit_transform(ppmiMat_2)\n",
    "plmiMat_2_tr300 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_300, algorithm = \"arpack\").fit_transform(plmiMat_2)\n",
    "chiMat_2_tr300 = sklearn.decomposition.TruncatedSVD(n_components = dimensions_300, algorithm = \"arpack\").fit_transform(chiMat_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All first 50 dimensions but with first 5 dimensions\n",
    "from scipy import linalg, dot\n",
    "dimensions_5 = 5\n",
    "\n",
    "# Now let's filter out all first 300 dimensions but the first 5\n",
    "W = U[:, 0 : dimensions]\n",
    "sigma = s[:dimensions]  # for efficiency reasons sigma is an array, not a matrix\n",
    "C = Vt[0 : dimensions, 0 : 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For stemmed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's decompose our matrix:\n",
    "U, s, Vt = linalg.svd(ppmiMat)  \n",
    "ppmiMat_tr300_alt = dot(W, linalg.diagsvd(sigma, dimensions, len(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's decompose our matrix:\n",
    "U, s, Vt = linalg.svd(plmiMat)  \n",
    "plmiMat_tr300_alt = dot(W, linalg.diagsvd(sigma, dimensions, len(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's decompose our matrix:\n",
    "U, s, Vt = linalg.svd(chiMat)  \n",
    "chiMat_tr300_alt = dot(W, linalg.diagsvd(sigma, dimensions, len(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For lemmatized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's decompose our matrix:\n",
    "U, s, Vt = linalg.svd(ppmiMat_2)  \n",
    "ppmiMat_2_tr300_alt = dot(W, linalg.diagsvd(sigma, dimensions, len(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's decompose our matrix:\n",
    "U, s, Vt = linalg.svd(plmiMat_2)  \n",
    "plmiMat_2_tr300_alt = dot(W, linalg.diagsvd(sigma, dimensions, len(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's decompose our matrix:\n",
    "U, s, Vt = linalg.svd(chiMat_2)  \n",
    "chiMat_2_tr300_alt = dot(W, linalg.diagsvd(sigma, dimensions, len(C)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 5. (10 points)\n",
    "\n",
    "Evaluate your models (with and without SVD) on a **Semantic Similarity Task**. And the winner is...\n",
    "\n",
    "\n",
    "Use SimLex-999 for this purpose.\n",
    "\n",
    "- the dataset is available in `data/SimLex-999.txt`\n",
    "\n",
    "\n",
    "- the dataset is described in `data/SimLex-999.README.txt`  (HINT: the relevant judgements are those in the `SimLex999` column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cosine simialarity on the models without SVD\n",
    "ppmiSimMat = sklearn.metrics.pairwise.cosine_similarity(ppmiMat)\n",
    "plmiSimMat = sklearn.metrics.pairwise.cosine_similarity(plmiMat)\n",
    "chiSimMat = sklearn.metrics.pairwise.cosine_similarity(chiMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine simialarity on the SVD models:\n",
    "#Dimension 50 \n",
    "#On stemmend models:\n",
    "ppmiSVD50SimMat = sklearn.metrics.pairwise.cosine_similarity(ppmiMat_tr50)\n",
    "plmiSVD50SimMat = sklearn.metrics.pairwise.cosine_similarity(plmiMat_tr50)\n",
    "chiSVD50SimMat = sklearn.metrics.pairwise.cosine_similarity(chiMat_tr50)\n",
    "\n",
    "#On lemmatized models:\n",
    "ppmi_2_SVD50SimMat = sklearn.metrics.pairwise.cosine_similarity(ppmiMat_2_tr50)\n",
    "plmi_2_SVD50SimMat = sklearn.metrics.pairwise.cosine_similarity(plmiMat_2_tr50)\n",
    "chi_2_SVD50SimMat = sklearn.metrics.pairwise.cosine_similarity(chiMat_2_tr50)\n",
    "\n",
    "#Dimension 300\n",
    "#On stemmend models:\n",
    "ppmiSVD300SimMat = sklearn.metrics.pairwise.cosine_similarity(ppmiMat_tr300)\n",
    "plmiSVD300SimMat = sklearn.metrics.pairwise.cosine_similarity(plmiMat_tr300)\n",
    "chiSVD300SimMat = sklearn.metrics.pairwise.cosine_similarity(chiMat_tr300)\n",
    "\n",
    "#On lemmatized models:\n",
    "plmi_2_SVD300SimMat = sklearn.metrics.pairwise.cosine_similarity(plmiMat_2_tr300)\n",
    "ppmi_2_SVD300SimMat = sklearn.metrics.pairwise.cosine_similarity(ppmiMat_2_tr300)\n",
    "chi_2_SVD300SimMat = sklearn.metrics.pairwise.cosine_similarity(chiMat_2_tr300)\n",
    "\n",
    "# Dimension 300 with first 5 excluded:\n",
    "#On stemmend models:\n",
    "ppmiSVD300SimMat_alt = sklearn.metrics.pairwise.cosine_similarity(ppmiMat_tr300_alt)\n",
    "plmiSVD300SimMat_alt = sklearn.metrics.pairwise.cosine_similarity(plmiMat_tr300_alt)\n",
    "chiSVD300SimMat_alt = sklearn.metrics.pairwise.cosine_similarity(chiMat_tr300_alt)\n",
    "\n",
    "#On lemmatized models:\n",
    "plmi_2_SVD300SimMat = sklearn.metrics.pairwise.cosine_similarity(plmiMat_2_tr300_alt)\n",
    "ppmi_2_SVD300SimMat_alt = sklearn.metrics.pairwise.cosine_similarity(ppmiMat_2_tr300_alt)\n",
    "chi_2_SVD300SimMat_alt = sklearn.metrics.pairwise.cosine_similarity(chiMat_2_tr300_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the pairs of nouns for which we have enough disitrbutional information\n",
    "\n",
    "SimLex999 = dict()\n",
    "\n",
    "\n",
    "with open(\"data/SimLex999.csv\",\"rb\") as infile:\n",
    "    infile.next()\n",
    "    for line in infile:\n",
    "        raw_w1, raw_w2, rating = line.strip().split(\",\")\n",
    "        w1 = raw_w1+\"-NOUN\"\n",
    "        w2 = raw_w2+\"-NOUN\"\n",
    "        \n",
    "        if all([vectors_indices.has_key(w) for w in [w1,w2]]):\n",
    "            SimLex999[(w1, w2)] = float(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positivePMI-based space vs. SimLex999 -> spearman's rho:\t\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "#Add all SVD values\n",
    "measure2mat = {\"positivePMI\": ppmiSimMat}\n",
    "#\"ppmi_SVD300\": ppmiSVD300SimMat\n",
    "# \"plmi_SVD300\": plmiSVD300SimMat, \"chi_SVD300\": chiSVD300SimMat, \"ppmi_2_SVD300\": ppmi_2_SVD300SimMat, \"plmi_2_SVD300\": plmi_2_SVD300SimMat, \"chi_2_SVD300\": chi_2_SVD300SimMat\n",
    "# \"positivePMI\": ppmiSimMat, \"positiveLMI\": plmiSimMat, \"ChiSquared\": chiSimMat,\n",
    "# \"ppmi_SVD50\": ppmiSVD50SimMat, \"plmi_SVD50\": plmiSVD50SimMat, \"chi_SVD50\": chiSVD50SimMat,\n",
    "# \"ppmi_2_SVD50\": ppmi_2_SVD50SimMat, \"plmi_2_SVD50\": plmi_2_SVD50SimMat, \"chi_2_SVD50\": chi_2_SVD50SimMat,\n",
    "\n",
    "rhos = []\n",
    "measures = []\n",
    "\n",
    "for m, mat in measure2mat.items():\n",
    "    print m+\"-based space vs. SimLex999 -> spearman's rho:\\t\" \n",
    "    \n",
    "    SimLex_ratings = []\n",
    "    vsm_sims = []\n",
    "    for (w1, w2), r in SimLex999.iteritems():\n",
    "        w1idx = vectors_indices[w1]\n",
    "        w2idx = vectors_indices[w2]\n",
    "        \n",
    "        SimLex_ratings.append(r)\n",
    "        vsm_sims.append(mat[w1idx, w2idx])\n",
    "        \n",
    "    rho, pval = scipy.stats.spearmanr(SimLex_ratings, vsm_sims)\n",
    "    \n",
    "    print rho\n",
    "    rhos.append(rho)\n",
    "    measures.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positivePMI': array([[ 1.        ,  0.        ,  0.07981903, ...,  0.0573352 ,\n",
      "         0.07794961,  0.06345896],\n",
      "       [ 0.        ,  1.        ,  0.02544241, ...,  0.        ,\n",
      "         0.04939597,  0.05389868],\n",
      "       [ 0.07981903,  0.02544241,  1.        , ...,  0.03337256,\n",
      "         0.09876506,  0.04402459],\n",
      "       ..., \n",
      "       [ 0.0573352 ,  0.        ,  0.03337256, ...,  1.        ,\n",
      "         0.06189858,  0.0761467 ],\n",
      "       [ 0.07794961,  0.04939597,  0.09876506, ...,  0.06189858,\n",
      "         1.        ,  0.1035627 ],\n",
      "       [ 0.06345896,  0.05389868,  0.04402459, ...,  0.0761467 ,\n",
      "         0.1035627 ,  1.        ]])}\n"
     ]
    }
   ],
   "source": [
    "print measure2mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGfCAYAAAC5sxM+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2v8DQIu5/QYY8SYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19399efd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(0, len(rhos)), rhos)\n",
    "plt.xticks(range(0, len(rhos)), measures, size='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 6. (15 points)\n",
    "\n",
    "Compare the performances of your models against those that can be obtained by using three WordNet-based similarity measures of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 7. (15 points)\n",
    "\n",
    "Train your best-perfoming model on those documents of the reuters corpus that are categorized as 'earn'. \n",
    "\n",
    "\n",
    "Compare the similairity scores obtained from the subset against those in the full dataset by choosing 15 words that are sufficently frequent in both corpora and\n",
    "\n",
    "- visualize their similiarty scores in each corpus by using either Multi Dimensional Scaling or hierarchical Clustering\n",
    "\n",
    "\n",
    "- describe explicitely how do you think things changed by having more data (i.e. from the subset to the whole corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
