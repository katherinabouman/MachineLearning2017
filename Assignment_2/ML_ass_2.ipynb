{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Lab Assignment 2: Evaluate classifiers (10 points)\n",
    "\n",
    "Student name: Katja Bouman\n",
    " \n",
    "In this assignment you will optimize and compare the perfomance of a parametric (logistic regression) and non-parametric (k-nearest neighbours) classifier on the MNIST dataset.\n",
    "\n",
    "Publish your notebook (ipynb file) to your Machine Learning repository on Github ON TIME. We will check the last commit on the day of the deadline.  \n",
    "\n",
    "### Deadline Friday, November 17, 23:59.\n",
    "\n",
    "This notebook consists of three parts: design, implementation, results & analysis. \n",
    "We provide you with the design of the experiment and you have to implement it and analyse the results.\n",
    "\n",
    "### Criteria used for grading\n",
    "* Explain and analyse all results.\n",
    "* Make your notebook easy to read. When you are finished take your time to review it!\n",
    "* You do not want to repeat the same chunks of code multiply times. If your need to do so, write a function. \n",
    "* The implementation part of this assignment needs careful design before you start coding. You could start by writing pseudocode.\n",
    "* In this exercise the insights are important. Do not hide them somewhere in the comments in the implementation, but put them in the Analysis part\n",
    "* Take care that all the figures and tables are well labeled and numbered so that you can easily refer to them.\n",
    "* A plot should have a title and axes labels.\n",
    "* You may find that not everything is 100% specified in this assignment. That is correct! Like in real life you probably have to make some choices. Motivate your choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading points distribution\n",
    "\n",
    "* Implementation 5 points\n",
    "* Results and analysis 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not have to keep the order of this design and are allowed to alter it if you are confident.\n",
    "* Import all necessary modules. Try to use as much of the available functions as possible. \n",
    "* Use the provided train and test set of MNIST dataset.\n",
    "* Pre-process data eg. normalize/standardize, reformat, etc.           \n",
    "  Do whatever you think is necessary and motivate your choices.\n",
    "* (1) Train logistic regression and k-nn using default settings.\n",
    "* Use 10-fold cross validation for each classifier to optimize the performance for one parameter: \n",
    "    * consult the documentation on how cross validation works in sklearn (important functions:             cross_val_score(), GridSearchCV()).\n",
    "    * Optimize k for k-nn,\n",
    "    * for logistic regression focus on the regularization parameter,\n",
    "* (2) Train logistic regression and k-nn using optimized parameters.\n",
    "* Show performance on the cross-validation set for (1) and (2) for both classifiers: \n",
    "    * report the average cross validation error rates (alternatively, the average accuracies - it's up to you) and standard deviation,\n",
    "    * plot the average cross valildation errors (or accuracies) for different values of the parameter that you tuned. \n",
    "* Compare performance on the test set for two classifiers:\n",
    "    * produce the classification report for both classifiers, consisting of precision, recall, f1-score. Explain and analyse the results.\n",
    "    * print confusion matrix for both classifiers and compare whether they missclassify the same  classes. Explain and analyse the results.\n",
    "* Discuss your results.\n",
    "* BONUS: only continue with this part if you are confident that your implemention is complete \n",
    "    * tune more parameters of logistic regression\n",
    "    * add additional classifiers (NN, Naive Bayes, decision tree), \n",
    "    * analyse additional dataset (ex. Iris dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['clf']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split #to split in train and test set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note: \n",
    "    I used python 2 ! So some things might go differently if you run my code in python 3 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load mnist dataset \n",
    "digits = load_digits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test set\n",
    "X_train = reshape(digits.images[:1500],(1500,64))\n",
    "X_test = reshape(digits.images[1500:],(297,64))\n",
    "y_train = digits.target[:1500]\n",
    "y_test = digits.target[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (1) Train logistic regression and k-nn using default settings\n",
    "\n",
    "#Pre-process data eg. normalize/standardize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "LR = LogisticRegression(max_iter = 200)\n",
    "\n",
    "#k-nn with default setting n=5\n",
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "# Training\n",
    "# Train both classifiers, uising fit() function.\n",
    "LR_fitted = LR.fit(X_train, y_train)\n",
    "KNN_fitted = KNN.fit(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best score for training data:', 0.9613333333333334)\n",
      "('Best k:', 3)\n",
      "('Best score for training data:', 0.944)\n",
      "('Best parameter:', 0.92)\n"
     ]
    }
   ],
   "source": [
    "#Use 10-fold cross validation for each classifier to optimize the performance for one parameter\n",
    "\n",
    "#For the knn finding the optimal k\n",
    "#Try k 0 to 6\n",
    "parameters = {'n_neighbors': [1, 2, 3, 4, 5, 6]}\n",
    "\n",
    "#Use the KNN from before with the the default k\n",
    "#Now use GridSearch to try all 20 k's, cv = 10 because we do 10-fold cross validation\n",
    "clfK = GridSearchCV(KNN, parameters,cv=10)\n",
    "\n",
    "#Now optimilize to find the best preforming k\n",
    "clfK.fit(X_train, y_train)\n",
    "\n",
    "#Print the best score and the optimal k\n",
    "print('Best score for training data:', clfK.best_score_) \n",
    "print('Best k:',clfK.best_estimator_.n_neighbors) \n",
    "\n",
    "\n",
    "#Now for the logistic regression\n",
    "#Set a range of parameters that you will try\n",
    "#parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] } this gave optimal = 1.0 so I specified it even more to:\n",
    "#parameters = {'C': [0.97, 0.98, 0.99, 1.0, 1.1] } this gave optimal = 0.97 so I specified it even more to:\n",
    "parameters = {'C': [0.92, 0.93, 0.94, 0.95, 0.96, 0.97]}\n",
    "\n",
    "#Use the LR from before with the default logistic regression with 200 iterations\n",
    "#Now use GridSearch to try all the parameters\n",
    "clf = GridSearchCV(LR, parameters, cv=10)\n",
    "\n",
    "#Now optimilize to find the best preforming parameter\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Print the best score and the optimal k\n",
    "print('Best score for training data:', clf.best_score_) \n",
    "print('Best parameter:',clf.best_estimator_.C) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#So now we will train logistic regression and k-nn using their optimized parameters.\n",
    "#The optimal value for knn and logistic regression on this data have shown to be 3 and 0.94 repectivly. \n",
    "\n",
    "# Logistic Regression\n",
    "LR_2 = LogisticRegression(C=0.92, max_iter = 200)\n",
    "\n",
    "#k-nn with default setting n=5\n",
    "KNN_2 = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Training\n",
    "# Train both classifiers, uising fit() function.\n",
    "LR_2_fitted = LR_2.fit(X_train, y_train)\n",
    "KNN_2_fitted = KNN_2.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression default accuracy and standard deviation: 0.924023894863 0.0159971232403 \n",
      "\n",
      "Knn default accuracy and standard deviation: 0.928701655573 0.0237358774393 \n",
      "\n",
      "Logistic regression optimal accuracy and standard deviation: 0.923357228196 0.0169163244827 \n",
      "\n",
      "Knn optimal accuracy and standard deviation: 0.93675499232 0.0179780612783 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now show performance on the cross-validation set for (1) and (2) for both classifiers\n",
    "# report the average cross validation error rates and standard deviation\n",
    "\n",
    "#Classifiers is a list of the different classifiers\n",
    "Classifiers = [LR, KNN, LR_2, KNN_2]\n",
    "\n",
    "#accuracy is a list with all the average cross validation error rates of the different classifiers\n",
    "accuracy = []\n",
    "#stdv is a list with all the standard deviations of the different classifiers\n",
    "stdv = []\n",
    "\n",
    "for i in range(len(Classifiers)):\n",
    "    score = cross_val_score(Classifiers[i], X_train, y_train)\n",
    "    accuracy.append(score.mean())\n",
    "    stdv.append(score.std())\n",
    "    \n",
    "    \n",
    "# (1) Default values logistic regression\n",
    "print\"Logistic regression default accuracy and standard deviation:\", accuracy[0], stdv[0], \"\\n\"\n",
    "# (1) Default values KNN\n",
    "print \"Knn default accuracy and standard deviation:\", accuracy[1], stdv[1], \"\\n\"\n",
    "# (2) Optimal values logistic regression\n",
    "print \"Logistic regression optimal accuracy and standard deviation:\", accuracy[2], stdv[2], \"\\n\"\n",
    "# (2) Optimal values KNN\n",
    "print \"Knn optimal accuracy and standard deviation:\", accuracy[3], stdv[3], \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11c81cc10>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c902a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEH+1nE4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c300a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the average cross valildation errors (or accuracies) for different values of the parameter that you tuned.\n",
    "#So not just the optimal parameter, but also other values to compare the difference in preformance\n",
    "\n",
    "parameter_values = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "cv_values = []\n",
    "for i in range(len(parameter_values)):\n",
    "    LR_3 = LogisticRegression(C=parameter_values[i], max_iter = 200)\n",
    "    cv_score_3 = cross_val_score(LR_3, X_train, y_train)\n",
    "    cv_values.append(mean(cv_score_3))\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "k_values = [1, 2, 3, 4, 5, 6, 7, 8 ,9]\n",
    "cvk_values = []\n",
    "for i in range(len(k_values)):\n",
    "    KNN_3 = KNeighborsClassifier(n_neighbors=k_values[i])\n",
    "    cv_score_4 = cross_val_score(KNN_3, X_train, y_train)\n",
    "    cvk_values.append(np.mean(cv_score_4))\n",
    "    i += 1\n",
    "    \n",
    "plt.figure(1)\n",
    "plt.title('Logistic Regression')\n",
    "plt.ylabel('cross validation score')\n",
    "plt.xlabel('parameter values')\n",
    "plt.plot(parameter_values, cv_values, 'ro')\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.ylabel('cross validation score')\n",
    "plt.xlabel('parameter values')\n",
    "plt.plot(k_values, cvk_values, 'ro')\n",
    "plt.title('Knn Classification')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression optimal classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.93      0.94        27\n",
      "          1       0.90      0.87      0.89        31\n",
      "          2       0.96      0.96      0.96        27\n",
      "          3       0.77      0.67      0.71        30\n",
      "          4       0.97      0.91      0.94        33\n",
      "          5       0.88      1.00      0.94        30\n",
      "          6       0.94      1.00      0.97        30\n",
      "          7       0.88      0.93      0.90        30\n",
      "          8       0.78      0.89      0.83        28\n",
      "          9       0.89      0.77      0.83        31\n",
      "\n",
      "avg / total       0.89      0.89      0.89       297\n",
      "\n",
      "Knn optimal classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        27\n",
      "          1       0.94      1.00      0.97        31\n",
      "          2       0.89      0.89      0.89        27\n",
      "          3       0.92      0.80      0.86        30\n",
      "          4       1.00      0.85      0.92        33\n",
      "          5       0.91      1.00      0.95        30\n",
      "          6       0.91      1.00      0.95        30\n",
      "          7       0.91      0.97      0.94        30\n",
      "          8       0.93      0.89      0.91        28\n",
      "          9       0.87      0.87      0.87        31\n",
      "\n",
      "avg / total       0.93      0.93      0.92       297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare performance on the test set for two classifiers\n",
    "# Use the function classification_report() from the sklearn metrics library \n",
    "#true values:\n",
    "\n",
    "# For the optimized logistic regression:\n",
    "# true values are:\n",
    "# y_test\n",
    "# predicted values are:\n",
    "# LR_2.predict(X_test)\n",
    "\n",
    "print \"Logistic regression optimal classification report:\", \"\\n\"\n",
    "print(classification_report(y_test, LR_2.predict(X_test)))\n",
    "\n",
    "# For the optimized knn:\n",
    "# true values are:\n",
    "# y_test\n",
    "# predicted values are:\n",
    "# KNN_2.predict(X_test)\n",
    "\n",
    "print \"Knn optimal classification report:\", \"\\n\"\n",
    "print(classification_report(y_test, KNN_2.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix of optimal Logistic regression:\n",
      " \n",
      "[[25  0  0  0  0  0  2  0  0  0]\n",
      " [ 0 27  0  2  0  0  0  0  2  0]\n",
      " [ 0  0 26  1  0  0  0  0  0  0]\n",
      " [ 0  1  1 20  0  2  0  3  3  0]\n",
      " [ 0  0  0  0 30  0  0  0  0  3]\n",
      " [ 0  0  0  0  0 30  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 30  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 28  1  0]\n",
      " [ 0  2  0  1  0  0  0  0 25  0]\n",
      " [ 1  0  0  2  0  2  0  1  1 24]]\n",
      " \n",
      "Confusion matrix of optimal Knn:\n",
      " \n",
      "[[27  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 31  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 24  0  0  0  3  0  0  0]\n",
      " [ 0  0  2 24  0  1  0  1  1  1]\n",
      " [ 0  1  0  0 28  0  0  1  1  2]\n",
      " [ 0  0  0  0  0 30  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 30  0  0  0]\n",
      " [ 0  0  1  0  0  0  0 29  0  0]\n",
      " [ 0  1  0  1  0  0  0  0 25  1]\n",
      " [ 0  0  0  1  0  2  0  1  0 27]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix for both classifiers and compare whether they missclassify the same classes. \n",
    "\n",
    "# For the optimized logistic regression:\n",
    "# true values are:\n",
    "# y_test\n",
    "# predicted values are:\n",
    "# LR_2.predict(X_test)\n",
    "print(\"Confusion matrix of optimal Logistic regression:\")\n",
    "print(\" \")\n",
    "print(confusion_matrix(y_test, LR_2.predict(X_test)))\n",
    "print(\" \")\n",
    "\n",
    "# For the optimized knn:\n",
    "# true values are:\n",
    "# y_test\n",
    "# predicted values are:\n",
    "# KNN_2.predict(X_test)\n",
    "print(\"Confusion matrix of optimal Knn:\")\n",
    "print(\" \")\n",
    "print(confusion_matrix(y_test, KNN_2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and analysis of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classification report\n",
    "\n",
    "    The classification report shows for each class (in our data digits) of the data the precision, recall and f-score of the classifier. The precicion shows for every digit that is predicted as true for a certain class, how many of these digits is actually classified correctly. The formula of precicion is: tp / (tp + fp), where tp means true positive and fp means false positive. So a low precision rate indicates a high false positive rate, which is very undesirable.\n",
    "    Recall identifies how many of the digits that actually belong to a certain class, are also classified true for that class. The formula is: tp/(tp + fn), where fn means false negative. In this case the number of false negative casses has influence on the rate. \n",
    "    F-score is the harmonic mean of the precision and the recall, with the complex formula: F-score = 2 * (PR / (P + R)). Where P stands for precision and R for recall. \n",
    "        For most real-life machine learning applications, a false positive is really bad. In for example the research that classified whether ex-verdict could return to the society. A false positive would result in the ex-verdict to return to society and again commit a crime. Thus the main goal for a classifier is to maximize true positive and minimize false positive.\n",
    "         However in my classification reports the scores of the precision and recall rates are the same for each specific classifier. You can however compare the two classifiers with each other and conclude that the k-nearest neighbour classifier has higher scores in all of the cases and thus has less false positives and less false negative.   \n",
    "            You can also investigate the behaviour of the classifiers per class. For the logistic regression classifier you can see that it preformed the worst on digit 3. For this digit both precision and recall were relatively low. The Knn classifier preformed worst on digit 2. \n",
    "         For both classifiers, Digit 4 is interesting because for the precision was excellent (at least for knn), but the recall was low. Which means that there were no false positive (it identified zero numbers wrongly as 4's), but quite some false negative (the classifier did not identify all 4's as 4's) And for digit 5 it was the other way around. This is also very clearly visible in the confusion matrixes which I will discuss next. \n",
    "    \n",
    "\n",
    "### Confusion matrix\n",
    "    \n",
    "    The confussion matrix is closely related with the classification report, however the martix gives a better insight in how the classifier classified every digit and thus gives a good view on where it went wrong. The rows represent the actual values of the digits and the columns represent the predicted value of the digits. So for example, the linear regression classifier identified 25 zero's correctly as zeros, but also falsly classified 2 zeros as 6's. The model also shows us that both classifies missclassified 3 multiple times and for a ranging number of different digits, where in contrast the logistic regression classifier missclassified for example 4 only for a 9.\n",
    "        \n",
    "        We can now use our findings from the classification report to further analyse the matrix and use these two to get a better view of our classifiers. First of all, we can see that as was also clear from the report, for logistic regression 3 has been often classified as an other digit than 3 and Knn was most often mistaken by 4. We can now also identify that 4 was most mistaken by a 9, by both classifiers, and 9 is multiple times falsly classifies as an 3 or a 5, also by both classifiers. \n",
    "        \n",
    "        The fact that our classifiers have difficulty with the same number could be an indication of a lack of those numbers in our training set. Our classifiers would then by overfitting the trainingset such that they are not preforming well on digits that are less presented in the used trainingset. The results from the confusion matrix could thus help to update our trainingset or regelize our model, to improve the preformance. \n",
    "        \n",
    "        Overall the k-nearest neigbourhood classifier preformed better than the Logistic Regression classifier. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
