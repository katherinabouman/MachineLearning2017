{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Graded Lab Assignment: Logistic Regression\n",
    "\n",
    "Student name : Katja Bouman \n",
    "\n",
    "Student number : 11155787\n",
    "\n",
    "In this assignment you will classify hand-written digits using logistic regression.\n",
    "\n",
    "The assignment follows Andrew Ng's explanation of Logistic Regression and (re)watching his videos could be useful (Week 3)\n",
    "\n",
    "Publish your notebook (ipynb file) to your Machine Learning repository on Github ON TIME. We will check the last commit on the day of the deadline. \n",
    "\n",
    "\n",
    "### Deadline Tuesday, October 10th, 23:59\n",
    "\n",
    "Do not hand in any other files, the Notebook should contain all your answers.\n",
    "\n",
    "The points for the assignmnet are distributed as follows:\n",
    "* The implementation\n",
    "    - prediction_function\n",
    "    - cost_function\n",
    "    - compute_gradient\n",
    "    - correct stop condition\n",
    "    - preventing overfitting\n",
    "    - systematically choosing learning rate \n",
    "    - \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a toolkit that has several datasets built in. You first need to install the toolkit: http://scikit-learn.org/stable/install.html\n",
    "\n",
    "The MNIST dataset that you will be using for this assignment contains images of hand-written digits that are only 8 by 8 pixels, which means the algorithm (logistic regression) should run on every computer.\n",
    "\n",
    "The code in the cell below shows how to work with the digits dataset and  how to visualize it. As you can see the numbers are not very clear in 8x8 pixels images, this means we cannot expect our logistic regression will have a very high classification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numbers shown are: \n",
      "[[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACoCAYAAADw6BWzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113a21950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=10)\n",
    "\n",
    "#Create two rows with numbers\n",
    "firstrow = np.hstack(digits.images[:5,:,:])\n",
    "secondrow = np.hstack(digits.images[5:10,:,:])\n",
    "\n",
    "plt.gray()\n",
    "plt.axis('off')\n",
    "\n",
    "#Show both rows at the same time\n",
    "plt.imshow(np.vstack((firstrow,secondrow)))\n",
    "\n",
    "print \"The numbers shown are: \\n\", np.vstack((digits.target[:5], digits.target[5:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The implementation\n",
    "\n",
    "You have to implement the next three functions and fill in the body of the loop in order to create a correct implementation of logistic regression. Don't change the definitions of the functions and input parameters.\n",
    "\n",
    "(1) Make sure that you do not overfit by keeping track of the score on the test set and implementing a correct stop condition. \n",
    "(2) Systematically pick a learning rate alpha that makes sure the algorithm learns in a smooth and stable manner (show how you do it). \n",
    "(3) Plot how your score on the test set improves over time. My best score was about 85% correct!\n",
    "(4) Make sure to comment your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hypothesis function takes x and theta and returns the predicted x, using dot product because theta and x are both vectors\n",
    "def prediction_function(x,theta):\n",
    "    z = dot(theta.transpose(),x)\n",
    "    return 1 / ( 1 + exp(-z))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The cost function J, takes the predicted x and takes a y vector and it returns the associated costs.\n",
    "# The y is transposed such that it has the right shape for the x value\n",
    "# The minus in front is because the log cause the cost to be negative, while its nicer to show it as a positive value\n",
    "def cost_function(x_predict,y): \n",
    "    m = size(y)\n",
    "    result = (-1/m) * sum(transpose(y) * log(x_predict)+transpose((1-y)) * log(1-x_predict))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The gradient function takes the predicted x, the y vector and the x values and returns the gradient valeus; the new theta values.\n",
    "def compute_gradient(x_predict, y, x):\n",
    "    m = len(x)\n",
    "    dtheta = (outer((x_predict - y),x))/m  \n",
    " \n",
    "    return dtheta.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nONDNzNqEA93MrE040M3M2sT/B6ve8k6Gi5nsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b5bb150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.5622895623\n"
     ]
    }
   ],
   "source": [
    "#Choose a suitable learning rate\n",
    "alpha = 0.001\n",
    "iterations = 500   \n",
    "theta = np.zeros((64,10))\n",
    "\n",
    "#Choose a suitable stopcondition\n",
    "stopcondition = 0.00005\n",
    "\n",
    "# Choose a x_test set and the right target_test\n",
    "x = np.reshape(digits.images[:1500],(1500,64))\n",
    "x_test = np.reshape(digits.images[1500:],(297,64))\n",
    "\n",
    "target = digits.target[:1500]\n",
    "target_test = digits.target[1500:]\n",
    "\n",
    "performance_array = []\n",
    "total_cost = []\n",
    "\n",
    "# Now we put everything together\n",
    "for i in range(iterations):\n",
    "    for j in range(x.shape[0]):\n",
    "        # Use the hypothesis function to create an predicted x\n",
    "        x_predict = prediction_function(x[j,:],theta)    \n",
    "            \n",
    "        # First we need a vector with only zeros, next we can set the right class equal to 1\n",
    "        y = np.zeros(10)\n",
    "        y[target[j]] = 1            \n",
    "        \n",
    "        cost = cost_function(x_predict,y)\n",
    "        # Make list of the cost at each iteration, then you can see how it changes\n",
    "        total_cost.append(cost)      \n",
    "        \n",
    "        # Gradient descent\n",
    "        gradient_value = compute_gradient(x_predict, y, x[j,:])\n",
    "           \n",
    "\n",
    "        # Update theta's using the gradient descent and the learning rate\n",
    "        theta -= alpha *gradient_value    \n",
    " \n",
    "          \n",
    "    # Now we need to keep track of how our algorithm is inproving by learning and whether it is improving it's score. \n",
    "    score = 0\n",
    "    for j in range(x_test.shape[0]):\n",
    "    # We use the test set as our input for x\n",
    "        x_predict = prediction_function(x_test[j,:].T, theta) \n",
    "    # And then compare if the values are equal to the (correct) ones in the target set \n",
    "        if argmax(x_predict) == target_test[j]:\n",
    "        # If algorithm finds the correct x, add one to your score of correct predictions\n",
    "            score += 1\n",
    "    # Now get your score in percentage (out of the whole test set what percentage did you guess right)\n",
    "    performance = (score / x_test.shape[0])*100\n",
    "    performance_array.append(performance)\n",
    "    \n",
    "    \n",
    "# Use the number of iterations as the x array for your plot           \n",
    "x_Array = range(iterations)\n",
    "\n",
    "# Now plot the percentage of correctly guessed images against the number of iterations. \n",
    "plt.plot(x_Array, performance_array)\n",
    "plt.show()\n",
    "\n",
    "#Keep track of your best performance:\n",
    "print max(performance_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 88.88888888888889)\n",
      "(700, 88.88888888888889)\n",
      "(900, 88.55218855218855)\n"
     ]
    }
   ],
   "source": [
    "# Choose a x_test set and the right target_test\n",
    " \n",
    "alpha = 0.001\n",
    "x = np.reshape(digits.images[:1500],(1500,64))\n",
    "x_test = np.reshape(digits.images[1500:],(297,64))\n",
    "\n",
    "target = digits.target[:1500]\n",
    "target_test = digits.target[1500:]\n",
    "\n",
    "#When I wanted to find my optimal alpha I used these lines instead of \"for iterations\" and just kept iterations constant\n",
    "#iterations = 500 \n",
    "#list_alpha = [0.0001, 0.001, 0.005]\n",
    "#for alpha in list_alpha:\n",
    "\n",
    "\n",
    "list_iterations = [500, 700, 900]    \n",
    "for iterations in list_iterations:  \n",
    "    performance_array = []\n",
    "    # Now we put everything together\n",
    "    for i in range(iterations):\n",
    "        for j in range(x.shape[0]):\n",
    "            # Use the hypothesis function to create an predicted x\n",
    "            x_predict = prediction_function(x[j,:],theta)    \n",
    "\n",
    "            # First we need a vector with only zeros, next we can set the right class equal to 1\n",
    "            y = np.zeros(10)\n",
    "            y[target[j]] = 1            \n",
    "\n",
    "            cost = cost_function(x_predict,y)\n",
    "            # Make list of the cost at each iteration, then you can see how it changes\n",
    "            total_cost.append(cost)      \n",
    "\n",
    "            # Gradient descent\n",
    "            gradient_value = compute_gradient(x_predict, y, x[j,:])\n",
    "\n",
    "\n",
    "            # Update theta's using the gradient descent and the learning rate\n",
    "            theta -= alpha *gradient_value    \n",
    "\n",
    "          \n",
    "        # Now we need to keep track of how our algorithm is inproving by learning and whether it is improving it's score. \n",
    "        score = 0\n",
    "        for j in range(x_test.shape[0]):\n",
    "        # We use the test set as our input for x\n",
    "            x_predict = prediction_function(x_test[j,:].T, theta) \n",
    "        # And then compare if the values are equal to the (correct) ones in the target set \n",
    "            if argmax(x_predict) == target_test[j]:\n",
    "            # If algorithm finds the correct x, add one to your score of correct predictions\n",
    "                score += 1\n",
    "        # Now get your score in percentage (out of the whole test set what percentage did you guess right)\n",
    "        performance = (score / x_test.shape[0])*100\n",
    "        performance_array.append(performance)\n",
    "\n",
    "\n",
    "    print (iterations, max(performance_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Check for overfitting \n",
    "It is important to check that you're not overfitting by testing your prediction on a testset. You can use the training error and the cross validation error for this. If the cross validation error and the training error are high (both almost the same value) then the bias is high, which means you are underfitting. If the cross validation error is high and the training error is low, then the varience is high, which means that you are overfitting.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best score analysis \n",
    "\n",
    "The score changes as you change the number of iterations and the value for alpha. Therefore you should first check which alpha gives the best score (see code above) and then with that alpha change the number of iterations and check which one gives you the best score. I started with iterations constant at 500 and trying different values for Alpha. Which showed that the optimal Alpha is 0.001.\n",
    "\n",
    "##### Trial 1: <br>\n",
    "\n",
    "Alpha = 0.0001\n",
    "\n",
    "Score = 89.23\n",
    "\n",
    "##### Trial 2:  <br>\n",
    "\n",
    "Alpha = 0.001    <br>\n",
    "\n",
    "Score = 90.23   <br>\n",
    "\n",
    "\n",
    "##### Trial 3:   <br>\n",
    "\n",
    "Alpha = 0.005   <br>\n",
    "\n",
    "Score = 88.88   <br>\n",
    "\n",
    "Changing the iterations turned out to not cause a significant improvement for the score. I kept the Alpha this time constant at 0.001, as that turned out to be the optimal alpha according to my findings. These were my results for the different iterations:\n",
    "\n",
    "##### Trial 4: <br>\n",
    "\n",
    "Iterations = 500\n",
    "\n",
    "Score = 88.88\n",
    "\n",
    "##### Trial 5:  <br>\n",
    "\n",
    "Iterations = 700    <br>\n",
    "\n",
    "Score = 88.88   <br>\n",
    "\n",
    "\n",
    "##### Trial 6:   <br>\n",
    "\n",
    "Iterations = 900   <br>\n",
    "\n",
    "Score = 88.55   <br>\n",
    "\n",
    "Showing that 500 and 700 give the same score. However 500 takes a lot less time, so I would argue that 500 is the optimal number of iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary ##\n",
    "\n",
    "In this assignment the goal was to make an algorithm that could used logistic regression for identifying numbers from images of hand-written digits. The algorithm includes a standard logistic regression hypothesis function, cost function and gradient descent function. All three in vectorized form. Next we used these functions to compare whether the hypotheses function predicted the correct x, and if not use the gradient descent to update the theta by multiplying it by a specific learningrate. And in this way learning and improving the number of correct classifications. The alpha and number of iterations that you choose has great influence on the result, so I have made a new cell where I used a for loop to check which alpha and what number of Iterations gave the highest score. Which turned out to be 500 iterations and an alpha of 0.001, giving a score of 90.23 % correct. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
